{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### insightface library 설치\n",
    "- pip install insightface\n",
    "- pip install onnxruntime #for CPU-only\n",
    "- pip install onnxruntime-gpu #For GPU\n",
    "### OpenCV 재설치\n",
    "- insightface 설치 시 opencv-python(또는 opencv-python-headless)가 자동 설치됨. uninstall 후 opencv-contrib-python 설치\n",
    "- pip uninstall opencv-python\n",
    "- pip uninstall opencv-python-headless\n",
    "- pip install opencv-contrib-python\n",
    "### AI Model - Facial Anaysis 모델 설치\n",
    "- buffalo_l download from : \n",
    "    https://github.com/deepinsight/insightface/releases\n",
    "\n",
    "- unzip buffalo_l.zip on `C:\\Users\\<user>\\.insightface\\models\\buffalo_l`\n",
    "- Facial Analysis 모델 : \n",
    "    - 얼굴 감지: 먼저 이미지에서 얼굴을 찾아내고, 얼굴의 위치 감지. 여러 얼굴이 있는 경우 각 얼굴의 위치를 정확하게 추출.\n",
    "    - 얼굴 특징 추출: 감지된 얼굴에서 고유한 특징(임베딩)을 추출하여 이를 벡터 형식으로 표현. 이 특징은 각 얼굴을 고유하게 나타내며, 다른 얼굴과 비교할 때 사용될 수 있음.\n",
    "- `buffalo_l` 모델이 insightface에서 사용할 수 있는 모델 중 정확도가 가장(그나마) 높음\n",
    "```\n",
    "Recognition Accuracy:\n",
    "\n",
    "+-------+-------+--------+-------+--------+--------+------+----+------+-------+\n",
    "| Name  | MR-ALL| African| Cauca | South  | East   | LFW  | CF | AgeD | IJB-C |\n",
    "|       |       |        | sian  | Asian  | Asian  |      | P- | B-30 | (E4)  |\n",
    "|       |       |        |       |        |        |      | FP |      |       |\n",
    "+=======+=======+========+=======+========+========+======+====+======+=======+\n",
    "| buffa | 91.25 | 90.29  | 94.70 | 93.16  | 74.96  | 99.83| 99 | 98.23| 97.25 |\n",
    "| lo_l  |       |        |       |        |        |      | .33|      |       |\n",
    "```\n",
    "### AI Model - Face Swap 모델 설치\n",
    "- Download `inswapper_128.onnx` & Locate it in a specific directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 얼굴 인식을 위해 InsightFace를 사용하는 샘플 코드\n",
    "\n",
    "import cv2\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# FaceAnalysis 객체 초기화 (사전 학습된 모델 사용)\n",
    "app = FaceAnalysis(name='buffalo_l')  # 'buffalo_l'는 사전 학습된 모델 이름입니다.\n",
    "app.prepare(ctx_id=-1)  # ctx_id=0은 GPU 사용, ctx_id=-1은 CPU 사용\n",
    "\n",
    "# NMS 임계값 설정\n",
    "# - 낮출수록 더 많은 얼굴이 검출될 수 있지만 오탐률이 증가할 수 있음\n",
    "app.det_model.nms_thresh = 0.6\n",
    "\n",
    "# 이미지 파일 읽기\n",
    "img = cv2.imread(\"./faces/bk.goldengirls01.jpg\")  # 처리할 이미지 파일의 경로로 변경하세요.\n",
    "if img is None:\n",
    "    raise FileNotFoundError(f\"이미지를 불러올 수 없습니다. 경로를 확인하세요\")\n",
    "\n",
    "# 얼굴 검출 및 임베딩 추출\n",
    "faces = app.get(img)\n",
    "\n",
    "# 검출된 얼굴 처리\n",
    "for idx, face in enumerate(faces):\n",
    "    # 얼굴 영역 표시\n",
    "    bbox = face.bbox.astype(int)\n",
    "    cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "    # 얼굴 임베딩 출력\n",
    "    #print(f\"얼굴 {idx+1} 임베딩 벡터:\\n{face.embedding}\")\n",
    "\n",
    "# 결과 이미지 표시\n",
    "cv2.imshow('Detection Result', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# FaceAnalysis 객체 초기화 (사전 학습된 모델 사용)\n",
    "app = FaceAnalysis(name='buffalo_l')  # 'buffalo_l'는 사전 학습된 모델 이름입니다.\n",
    "app.prepare(ctx_id=0)  # ctx_id=0은 GPU 사용, ctx_id=-1은 CPU 사용\n",
    "\n",
    "# 비교할 기준 얼굴 이미지 로드 및 임베딩 추출\n",
    "ref_img_path = \"./faces/hanni01.jpg\"  # 기준 얼굴 이미지 경로로 변경하세요.\n",
    "ref_img = cv2.imread(ref_img_path)\n",
    "if ref_img is None:\n",
    "    raise FileNotFoundError(f\"기준 이미지를 불러올 수 없습니다. 경로를 확인하세요: {ref_img_path}\")\n",
    "\n",
    "ref_faces = app.get(ref_img)\n",
    "if len(ref_faces) == 0:\n",
    "    raise ValueError(\"기준 이미지에서 얼굴을 검출하지 못했습니다.\")\n",
    "\n",
    "# 기준 얼굴의 임베딩 추출 (첫 번째 얼굴 사용)\n",
    "ref_embedding = ref_faces[0].embedding\n",
    "\n",
    "# 비교할 대상 이미지 로드 및 얼굴 임베딩 추출\n",
    "target_img_path = \"./faces/newJeans_01.jpg\"  # 대상 이미지 경로로 변경하세요.\n",
    "target_img = cv2.imread(target_img_path)\n",
    "if target_img is None:\n",
    "    raise FileNotFoundError(f\"대상 이미지를 불러올 수 없습니다. 경로를 확인하세요: {target_img_path}\")\n",
    "\n",
    "target_faces = app.get(target_img)\n",
    "\n",
    "# target_faces를 x축 기준으로 정렬 (좌에서 우로)\n",
    "target_faces.sort(key=lambda face: face.bbox[0])\n",
    "\n",
    "# 검출된 얼굴들에 대해 유사도 계산 및 표시\n",
    "for idx, face in enumerate(target_faces):\n",
    "    # 대상 얼굴의 임베딩 추출\n",
    "    target_embedding = face.embedding\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    similarity = cosine_similarity([ref_embedding], [target_embedding])[0][0]\n",
    "\n",
    "    # 유사도 출력\n",
    "    print(f\"얼굴 {idx} 유사도: {similarity:.4f}\")\n",
    "\n",
    "    # 얼굴 영역 표시\n",
    "    bbox = face.bbox.astype(int)\n",
    "    cv2.rectangle(target_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "    # 유사도 텍스트 표시\n",
    "    cv2.putText(target_img, f\"{idx} : {similarity:.2f}\", (bbox[0], bbox[1]-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# 결과 이미지 표시\n",
    "cv2.imshow('Similarity Result', target_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Face Swapping\n",
    "##### 3-1. Class FaceSwapper\n",
    "- `ndarray` Type image에 Face Swap 를 수행하는 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.data import get_image as ins_get_image\n",
    "\n",
    "assert insightface.__version__ >= '0.7'\n",
    "\n",
    "SWAPPER_MODLE = 'C:\\\\Users\\\\tanmi\\\\stable-diffusion-webui\\\\models\\\\insightface\\\\inswapper_128.onnx'\n",
    "\n",
    "class FaceSwapper:\n",
    "    def __init__(self, model_name='buffalo_l', ctx_id=0, det_size=(640, 640)):\n",
    "        # 얼굴 분석 모델 초기화\n",
    "        self.app = FaceAnalysis(name=model_name)\n",
    "        self.app.prepare(ctx_id=ctx_id, det_size=det_size)\n",
    "        # 얼굴 교체 모델 로드\n",
    "        self.swapper = insightface.model_zoo.get_model(\n",
    "            SWAPPER_MODLE, download=True, download_zip=True\n",
    "        )\n",
    "        # 소스 얼굴 초기화\n",
    "        self.source_face = None\n",
    "        self.enhanced=False\n",
    "\n",
    "    def set_source_face(self, img, face_index=0, enhanced=False):\n",
    "        \"\"\"\n",
    "        이미지에서 소스 얼굴을 설정합니다.\n",
    "        img: 이미지 파일 경로나 numpy.ndarray 이미지\n",
    "        face_index: 선택할 얼굴의 인덱스 (기본값: 0)\n",
    "        \"\"\"\n",
    "        # 이미지 로드 (파일 경로 또는 ndarray 처리)\n",
    "        if isinstance(img, str):\n",
    "            img = cv2.imread(img)\n",
    "            if img is None:\n",
    "                print(f\"이미지를 로드할 수 없습니다: {img}\")\n",
    "                return False\n",
    "        elif not isinstance(img, np.ndarray):\n",
    "            print(\"유효한 이미지 또는 이미지 경로를 입력해 주세요.\")\n",
    "            return False\n",
    "\n",
    "        # 얼굴 검출\n",
    "        faces = self.app.get(img)\n",
    "        if len(faces) == 0:\n",
    "            print(\"소스 이미지에서 얼굴을 감지하지 못했습니다.\")\n",
    "            return False\n",
    "        # 얼굴을 x 좌표 기준으로 정렬\n",
    "        faces = sorted(faces, key=lambda x: x.bbox[0])\n",
    "        if face_index >= len(faces):\n",
    "            print(f\"소스 얼굴 인덱스가 범위를 벗어났습니다. 총 감지된 얼굴 수: {len(faces)}\")\n",
    "            return False\n",
    "        # 소스 얼굴 설정\n",
    "        self.source_face = faces[face_index]\n",
    "        print(f\"소스 얼굴이 설정되었습니다. 인덱스: {face_index}\")\n",
    "\n",
    "        self.enhanced=enhanced\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def enhance_image(self, img):\n",
    "        # 샤프닝\n",
    "        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "        sharpened = cv2.filter2D(img, -1, kernel)\n",
    "        \n",
    "        # 노이즈 제거\n",
    "        denoised = cv2.fastNlMeansDenoisingColored(sharpened, None, 10, 10, 7, 21)\n",
    "        \n",
    "        # 대비 향상\n",
    "        lab = cv2.cvtColor(denoised, cv2.COLOR_BGR2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        enhanced_lab = cv2.merge((cl,a,b))\n",
    "        enhanced = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        return enhanced\n",
    "    \n",
    "    def swap_faces_in_image(self, img):\n",
    "        \"\"\"\n",
    "        ndarray 이미지를 입력으로 받아 얼굴 교체를 수행하고, 결과 이미지를 반환합니다.\n",
    "        \"\"\"\n",
    "        if self.source_face is None:\n",
    "            print(\"소스 얼굴이 설정되지 않았습니다. 먼저 set_source_face 메서드를 호출하여 소스 얼굴을 설정하세요.\")\n",
    "            return None\n",
    "        # 얼굴 검출\n",
    "        faces = self.app.get(img)\n",
    "        if len(faces) == 0:\n",
    "            print(\"대상 이미지에서 얼굴이 감지되지 않았습니다.\")\n",
    "            return None\n",
    "        # 얼굴을 x 좌표 기준으로 정렬\n",
    "        faces = sorted(faces, key=lambda x: x.bbox[0])\n",
    "        # 얼굴 교체 수행\n",
    "        res = img.copy()\n",
    "        for face in faces:\n",
    "            res = self.swapper.get(res, face, self.source_face, paste_back=True)\n",
    "        \n",
    "        # 이미지 품질 향상\n",
    "        if self.enhanced:\n",
    "            res = self.enhance_image(res)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def extract_and_swap_faces_in_image(self, img):\n",
    "        \"\"\"\n",
    "        ndarray 이미지를 입력으로 받아 개별 얼굴을 교체한 이미지를 반환합니다.\n",
    "        \"\"\"\n",
    "        if self.source_face is None:\n",
    "            print(\"소스 얼굴이 설정되지 않았습니다. 먼저 set_source_face 메서드를 호출하여 소스 얼굴을 설정하세요.\")\n",
    "            return None\n",
    "        # 얼굴 검출\n",
    "        faces = self.app.get(img)\n",
    "        if len(faces) == 0:\n",
    "            print(\"대상 이미지에서 얼굴이 감지되지 않았습니다.\")\n",
    "            return None\n",
    "        # 얼굴을 x 좌표 기준으로 정렬\n",
    "        faces = sorted(faces, key=lambda x: x.bbox[0])\n",
    "        # 개별 얼굴 교체 및 추출\n",
    "        res = []\n",
    "        for face in faces:\n",
    "            _img, _ = self.swapper.get(img, face, self.source_face, paste_back=False)\n",
    "            res.append(_img)\n",
    "        if len(res) == 0:\n",
    "            print(\"교체된 얼굴이 없습니다.\")\n",
    "            return None\n",
    "        res = np.concatenate(res, axis=1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Face Swap Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_swapper = FaceSwapper(det_size=(320, 320))\n",
    "\n",
    "# 소스 얼굴 이미지 로드\n",
    "source_img = cv2.imread(\"./faces/hanni01.jpg\")\n",
    "    \n",
    "# 소스 얼굴 설정 (face_index는 선택 사항)\n",
    "success = face_swapper.set_source_face(source_img, face_index=0)\n",
    "if not success:\n",
    "    print(\"소스 얼굴 설정에 실패했습니다.\")\n",
    "    exit()\n",
    "    \n",
    "# 대상 이미지 로드\n",
    "target_img = cv2.imread('./faces/kimhs.jpg')\n",
    "    \n",
    "# 얼굴 교체 수행 (ndarray 이미지를 입력으로 받아 결과를 ndarray로 반환)\n",
    "swapped_img = face_swapper.swap_faces_in_image(target_img)\n",
    "\n",
    "if swapped_img is not None:\n",
    "    # 결과 이미지 표시\n",
    "    cv2.imshow('Similarity Result', swapped_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()    \n",
    "else:\n",
    "    print(\"얼굴 교체에 실패했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-2. Class VideoFaceSwapper\n",
    "- Video상의 특정 인물의 Face에 대하 Swap 를 수행하는 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class VideoFaceSwapper:\n",
    "    def __init__(self, \n",
    "            base_image, \n",
    "            target_video, \n",
    "            tolerance=0.35, \n",
    "            output_video=None, \n",
    "            display_video=True, \n",
    "            display_rectangle=True, \n",
    "            segments=None,\n",
    "            ctx_id=-1,\n",
    "            ):\n",
    "\n",
    "        # Initialize FaceAnalysis object\n",
    "        self.app = FaceAnalysis(name='buffalo_l')\n",
    "        self.app.prepare(ctx_id=ctx_id)  # 0 Use GPU (set ctx_id=-1 to use CPU)\n",
    "\n",
    "        # Load the reference face image and extract embedding\n",
    "        ref_img = cv2.imread(base_image)\n",
    "        if ref_img is None:\n",
    "            raise FileNotFoundError(f\"Unable to load the reference image: {base_image}\")\n",
    "\n",
    "        ref_faces = self.app.get(ref_img)\n",
    "        if len(ref_faces) == 0:\n",
    "            raise ValueError(\"No faces detected in the reference image.\")\n",
    "\n",
    "        # Extract embedding of the reference face (use the first face)\n",
    "        self.known_face_embedding = ref_faces[0].embedding\n",
    "        self.target_video = target_video\n",
    "        self.output_video = output_video\n",
    "        self.display_video = display_video\n",
    "        self.display_rectangle = display_rectangle\n",
    "        self.tolerance = tolerance\n",
    "        self.specific_person_present = False  # Flag to indicate if Specific Person is present\n",
    "\n",
    "        # Segments to process\n",
    "        self.segments = self._prepare_segments(segments)\n",
    "        \n",
    "        # If output_video is None, do not use video saving feature\n",
    "        self.fourcc = self._get_video_codec(output_video)\n",
    "\n",
    "        self.trackers = []\n",
    "        self.face_names = []\n",
    "        self.face_similarities = []\n",
    "\n",
    "    def _get_video_codec(self, output_video):\n",
    "        if output_video is None:\n",
    "            return None\n",
    "        _, ext = os.path.splitext(output_video.lower())\n",
    "        return cv2.VideoWriter_fourcc(*'VP90') if ext == '.webm' else cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        \n",
    "    def _convert_to_frame_range(self, start_time_str, duration, fps):\n",
    "        start_seconds = self._time_str_to_seconds(start_time_str)\n",
    "        end_seconds = start_seconds + duration\n",
    "        start_frame = int(start_seconds * fps)\n",
    "        end_frame = int(end_seconds * fps)\n",
    "        return start_frame, end_frame\n",
    "        \n",
    "    def _prepare_segments(self, segments):\n",
    "        if segments is None:\n",
    "            return None\n",
    "        segment_frames = []\n",
    "        fps = self.get_video_fps()\n",
    "        for start_time_str, duration in segments:\n",
    "            start_frame, end_frame = self._convert_to_frame_range(start_time_str, duration, fps)\n",
    "            segment_frames.append((start_frame, end_frame))\n",
    "        return segment_frames\n",
    "\n",
    "    def _get_video_properties(self, video_capture):\n",
    "        fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        return fps, width, height, total_frames\n",
    "\n",
    "    def _initialize_video_writer(self, fps, width, height):\n",
    "        if self.output_video:\n",
    "            return cv2.VideoWriter(self.output_video, self.fourcc, fps, (width, height))\n",
    "        return None\n",
    "\n",
    "    def _calculate_similarities(self, faces):\n",
    "        return [cosine_similarity([self.known_face_embedding], [face.embedding])[0][0] for face in faces]\n",
    "\n",
    "    def _create_tracker(self, frame, bbox):\n",
    "        tracker = cv2.legacy.TrackerKCF_create()\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        tracker_bbox = (x1, y1, x2 - x1, y2 - y1)\n",
    "        tracker.init(frame, tracker_bbox)\n",
    "        return tracker\n",
    "\n",
    "    def video_swap(self, func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Video capture\n",
    "            video_capture = cv2.VideoCapture(self.target_video)\n",
    "\n",
    "            # Get video properties\n",
    "            fps, width, height, total_frames = self._get_video_properties(video_capture)\n",
    "\n",
    "            video_writer = self._initialize_video_writer(fps, width, height)\n",
    "\n",
    "            # Initialize variables\n",
    "            self.trackers = []\n",
    "            self.face_names = []\n",
    "            self.face_similarities = []\n",
    "            frame_skip = 24\n",
    "            frame_count = 0\n",
    "\n",
    "            # Convert segments to list of (start_frame, end_frame)\n",
    "            segment_frames = self.segments if self.segments else [(0, total_frames)]\n",
    "\n",
    "            # Process each segment\n",
    "            for start_frame, end_frame in segment_frames:\n",
    "\n",
    "                if start_frame >= total_frames:\n",
    "                    print(f\"Start frame {start_frame} exceeds total frames {total_frames}. Skipping segment.\")\n",
    "                    continue\n",
    "\n",
    "                # Adjust end_frame if it exceeds total_frames\n",
    "                if end_frame > total_frames:\n",
    "                    end_frame = total_frames\n",
    "\n",
    "                # Set video capture to the start frame\n",
    "                video_capture.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "                frame_count = start_frame\n",
    "\n",
    "                while frame_count < end_frame:\n",
    "                    ret, frame = video_capture.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "\n",
    "                    frame_count += 1\n",
    "\n",
    "                    if not self.specific_person_present:\n",
    "                        # Attempt to detect Specific Person in every frame\n",
    "                        self.trackers = []\n",
    "                        self.face_names = []\n",
    "                        self.face_similarities = []\n",
    "\n",
    "                        # Detect faces and extract embeddings\n",
    "                        faces = self.app.get(frame)\n",
    "                        \n",
    "                        if len(faces) == 0:\n",
    "                            continue  # No faces detected, skip to next frame\n",
    "\n",
    "                        similarities = []\n",
    "                        for face in faces:\n",
    "                            face_embedding = face.embedding\n",
    "\n",
    "                            # Calculate cosine similarity\n",
    "                            similarity = cosine_similarity(\n",
    "                                [self.known_face_embedding], [face_embedding]\n",
    "                            )[0][0]\n",
    "                            similarities.append(similarity)\n",
    "\n",
    "                        # Find the index of the Specific Person\n",
    "                        specific_person_index = None\n",
    "                        if len(similarities) > 0:\n",
    "                            max_similarity = max(similarities)\n",
    "                            if max_similarity > self.tolerance:\n",
    "                                specific_person_index = similarities.index(max_similarity)\n",
    "\n",
    "                        #print(f\"face count : {len(faces)} similarities : {similarities} specific_person_index : {specific_person_index}\")\n",
    "                        \n",
    "                        if specific_person_index is not None:\n",
    "                            # Specific Person detected\n",
    "                            self.specific_person_present = True\n",
    "\n",
    "                            self._initialize_trackers(faces, frame, similarities, specific_person_index, func)\n",
    "                        else:\n",
    "                            # Specific Person not detected, process Unknown faces\n",
    "                            self.specific_person_present = False  # Ensure the flag is False\n",
    "\n",
    "                            for idx_face, face in enumerate(faces):\n",
    "                                bbox = face.bbox.astype(int)\n",
    "                                similarity = similarities[idx_face]\n",
    "\n",
    "                                if similarity > self.tolerance:\n",
    "                                    name = \"Candidate\"\n",
    "                                else:\n",
    "                                    name = \"Unknown\"\n",
    "\n",
    "                                # Since we are not tracking, we do not initialize trackers\n",
    "                                # Annotate frame without applying face swap\n",
    "                                x1, y1, x2, y2 = bbox\n",
    "                                tracker_bbox = (x1, y1, x2 - x1, y2 - y1)\n",
    "                                \n",
    "                                # Pass func=None to indicate no face swap should be applied\n",
    "                                self._annotate_frame(tracker_bbox, frame, name, similarity, func=None)\n",
    "                    else:\n",
    "                        # Specific Person is being tracked\n",
    "                        if frame_count % frame_skip == 0:\n",
    "                            # Re-detect faces\n",
    "                            self.trackers = []\n",
    "                            self.face_names = []\n",
    "                            self.face_similarities = []\n",
    "\n",
    "                            faces = self.app.get(frame)\n",
    "                            if len(faces) == 0:\n",
    "                                self.specific_person_present = False\n",
    "                                continue\n",
    "\n",
    "                            similarities = self._calculate_similarities(faces)\n",
    "\n",
    "                            # Find the index of the Specific Person\n",
    "                            specific_person_index = None\n",
    "                            if len(similarities) > 0:\n",
    "                                max_similarity = max(similarities)\n",
    "                                if max_similarity > self.tolerance:\n",
    "                                    specific_person_index = similarities.index(max_similarity)\n",
    "                                else:\n",
    "                                    specific_person_index = None\n",
    "\n",
    "                            if specific_person_index is not None:\n",
    "                                # Specific Person still detected\n",
    "                                self._initialize_trackers(faces, frame, similarities, specific_person_index, func)\n",
    "                            else:\n",
    "                                # Specific Person lost\n",
    "                                self.specific_person_present = False                                \n",
    "                        else:\n",
    "                            # Update trackers\n",
    "                            new_trackers = []\n",
    "                            new_face_names = []\n",
    "                            new_face_similarities = []\n",
    "                            specific_person_still_present = False\n",
    "\n",
    "                            for tracker, name, similarity in zip(self.trackers, self.face_names, self.face_similarities):\n",
    "                                success, tracker_bbox = tracker.update(frame)\n",
    "                                if success:\n",
    "                                    tracker_bbox = tuple(map(int, tracker_bbox))\n",
    "                                    new_trackers.append(tracker)\n",
    "                                    new_face_names.append(name)\n",
    "                                    new_face_similarities.append(similarity)\n",
    "\n",
    "                                    # Annotate frame and apply face swap if needed\n",
    "                                    self._annotate_frame(tracker_bbox, frame, name, similarity, func)\n",
    "\n",
    "                                    if name == \"Specific Person\":\n",
    "                                        specific_person_still_present = True\n",
    "                                else:\n",
    "                                    if name == \"Specific Person\":\n",
    "                                        specific_person_still_present = False\n",
    "\n",
    "                            # Update trackers and face info\n",
    "                            self.trackers = new_trackers\n",
    "                            self.face_names = new_face_names\n",
    "                            self.face_similarities = new_face_similarities\n",
    "\n",
    "                            if not specific_person_still_present:\n",
    "                                # Specific Person lost during tracking\n",
    "                                self.specific_person_present = False\n",
    "                                self.trackers = []\n",
    "                                self.face_names = []\n",
    "                                self.face_similarities = []\n",
    "\n",
    "                    # Display current frame number / total frames at the top-left corner\n",
    "                    cv2.putText(frame, f\"Frame: {frame_count}/{total_frames}\", (10, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "                    # Output or display video\n",
    "                    if self.display_video:\n",
    "                        cv2.imshow('Video', frame)\n",
    "                        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                            break\n",
    "\n",
    "                    if video_writer:\n",
    "                        video_writer.write(frame)\n",
    "\n",
    "            # Cleanup\n",
    "            video_capture.release()\n",
    "            if video_writer:\n",
    "                video_writer.release()\n",
    "            if self.display_video:\n",
    "                cv2.destroyAllWindows()\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    def get_video_fps(self):\n",
    "        video_capture = cv2.VideoCapture(self.target_video)\n",
    "        return video_capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    def _initialize_trackers(self, faces, frame, similarities, specific_person_index, func):\n",
    "\n",
    "        for idx, face in enumerate(faces):\n",
    "\n",
    "            bbox = face.bbox.astype(int)\n",
    "\n",
    "            if similarities[idx] > self.tolerance:                \n",
    "                name = \"Specific Person\" if idx == specific_person_index else \"Candidate\"\n",
    "            else:\n",
    "                name = \"Unknown\"\n",
    "\n",
    "            # Initialize tracker\n",
    "            tracker = self._create_tracker(frame, bbox)\n",
    "                                \n",
    "            self.trackers.append(tracker)\n",
    "            self.face_names.append(name)\n",
    "            self.face_similarities.append(similarities[idx])\n",
    "\n",
    "            # Annotate frame and apply face swap if needed\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            tracker_bbox = (x1, y1, x2 - x1, y2 - y1)\n",
    "            \n",
    "            self._annotate_frame(tracker_bbox, frame, name, similarities[idx], func)\n",
    "\n",
    "    def _annotate_frame(self, bbox, frame, name, similarity, func):\n",
    "\n",
    "        left, top, width, height = map(int, bbox)\n",
    "        expand_ratio = 0.3\n",
    "        expand_width = int(width * expand_ratio)\n",
    "        expand_height = int(height * expand_ratio)\n",
    "\n",
    "        expanded_left = int(max(0, left - expand_width))\n",
    "        expanded_top = int(max(0, top - expand_height))\n",
    "\n",
    "        frame_height, frame_width, _ = frame.shape\n",
    "        expanded_right = int(min(frame_width, left + width + expand_width))\n",
    "        expanded_bottom = int(min(frame_height, top + height + expand_height))\n",
    "\n",
    "        # Extract face region\n",
    "        face_region = frame[expanded_top:expanded_bottom, expanded_left:expanded_right]\n",
    "\n",
    "        if name == \"Specific Person\" and func:\n",
    "            # Apply face swap\n",
    "            swap_image = func(face_region)\n",
    "            # Replace the face region with the swapped image\n",
    "            if swap_image is not None:\n",
    "                swap_image_resized = cv2.resize(swap_image, (expanded_right - expanded_left, expanded_bottom - expanded_top))\n",
    "                frame[expanded_top:expanded_bottom, expanded_left:expanded_right] = swap_image_resized\n",
    "            color = (0, 0, 255)  # Red\n",
    "        elif name == \"Candidate\":\n",
    "            color = (255, 0, 0)  # Blue\n",
    "        else:\n",
    "            color = (0, 255, 0)  # Green\n",
    "\n",
    "        if self.display_rectangle:\n",
    "            # Draw rectangle and annotations\n",
    "            cv2.rectangle(frame, (expanded_left, expanded_top), (expanded_right, expanded_bottom), color, 2)\n",
    "            cv2.putText(frame, name, (expanded_left, expanded_bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "            cv2.putText(frame, f\"Similarity: {similarity:.2f}\", (expanded_left, expanded_bottom + 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    def _time_str_to_seconds(self, time_str):\n",
    "        # Convert \"mm:ss\" format to total seconds\n",
    "        minutes, seconds = map(int, time_str.split(':'))\n",
    "        total_seconds = minutes * 60 + seconds\n",
    "        return total_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class VideoFaceSwapper:\n",
    "    def __init__(self, base_image, target_video, tolerance=0.35, output_video=None, display_video=True, display_rectangle=True, segments=None, ctx_id=-1):\n",
    "        self.app = FaceAnalysis(name='buffalo_l')\n",
    "        self.app.prepare(ctx_id=ctx_id)\n",
    "\n",
    "        ref_img = cv2.imread(base_image)\n",
    "        if ref_img is None:\n",
    "            raise FileNotFoundError(f\"Unable to load the reference image: {base_image}\")\n",
    "\n",
    "        ref_faces = self.app.get(ref_img)\n",
    "        if len(ref_faces) == 0:\n",
    "            raise ValueError(\"No faces detected in the reference image.\")\n",
    "        \n",
    "        self.known_face_embedding = ref_faces[0].embedding\n",
    "        self.target_video = target_video\n",
    "        self.output_video = output_video\n",
    "        self.display_video = display_video\n",
    "        self.display_rectangle = display_rectangle\n",
    "        self.tolerance = tolerance\n",
    "        self.specific_person_present = False\n",
    "\n",
    "        self.segments = self._prepare_segments(segments)\n",
    "        self.fourcc = self._get_video_codec(output_video)\n",
    "\n",
    "        # Initialize the trackers, face names, and face similarities lists\n",
    "        self.trackers = []\n",
    "        self.face_names = []\n",
    "        self.face_similarities = []\n",
    "        \n",
    "    def _prepare_segments(self, segments):\n",
    "        if segments is None:\n",
    "            return None\n",
    "        segment_frames = []\n",
    "        fps = self.get_video_fps()\n",
    "        for start_time_str, duration in segments:\n",
    "            start_frame, end_frame = self._convert_to_frame_range(start_time_str, duration, fps)\n",
    "            segment_frames.append((start_frame, end_frame))\n",
    "        return segment_frames\n",
    "\n",
    "    def _get_video_codec(self, output_video):\n",
    "        if output_video is None:\n",
    "            return None\n",
    "        _, ext = os.path.splitext(output_video.lower())\n",
    "        return cv2.VideoWriter_fourcc(*'VP90') if ext == '.webm' else cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    def get_video_fps(self):\n",
    "        video_capture = cv2.VideoCapture(self.target_video)\n",
    "        return video_capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    def video_swap(self, func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            video_capture = cv2.VideoCapture(self.target_video)\n",
    "            fps, width, height, total_frames = self._get_video_properties(video_capture)\n",
    "            video_writer = self._initialize_video_writer(fps, width, height)\n",
    "\n",
    "            segment_frames = self.segments if self.segments else [(0, total_frames)]\n",
    "            for start_frame, end_frame in segment_frames:\n",
    "                if start_frame >= total_frames:\n",
    "                    continue\n",
    "                video_capture.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "                self._process_frames(video_capture, start_frame, end_frame, total_frames, fps, func, video_writer)\n",
    "\n",
    "            video_capture.release()\n",
    "            if video_writer:\n",
    "                video_writer.release()\n",
    "            if self.display_video:\n",
    "                cv2.destroyAllWindows()\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    def _get_video_properties(self, video_capture):\n",
    "        fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        return fps, width, height, total_frames\n",
    "\n",
    "    def _initialize_video_writer(self, fps, width, height):\n",
    "        if self.output_video:\n",
    "            return cv2.VideoWriter(self.output_video, self.fourcc, fps, (width, height))\n",
    "        return None\n",
    "\n",
    "    def _process_frames(self, video_capture, start_frame, end_frame, total_frames, fps, func, video_writer):\n",
    "        frame_count = start_frame\n",
    "        while frame_count < end_frame:\n",
    "            ret, frame = video_capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "\n",
    "            if self.specific_person_present:\n",
    "                self._track_or_detect_faces(frame, frame_count, total_frames, fps, func)\n",
    "            else:\n",
    "                self._detect_faces_in_frame(frame, func)\n",
    "\n",
    "            self._display_or_save_frame(frame, video_writer, frame_count, total_frames)\n",
    "\n",
    "    def _track_or_detect_faces(self, frame, frame_count, total_frames, fps, func):\n",
    "        frame_skip = 24\n",
    "        if frame_count % frame_skip == 0:\n",
    "            self._detect_faces_in_frame(frame, func)\n",
    "        else:\n",
    "            self._update_trackers(frame, func)\n",
    "\n",
    "    def _detect_faces_in_frame(self, frame, func):\n",
    "        faces = self.app.get(frame)\n",
    "        if len(faces) == 0:\n",
    "            self.specific_person_present = False\n",
    "            return\n",
    "\n",
    "        similarities = self._calculate_similarities(faces)\n",
    "        specific_person_index = self._find_specific_person_index(similarities)\n",
    "        if specific_person_index is not None:\n",
    "            self.specific_person_present = True\n",
    "            self._initialize_trackers(faces, frame, similarities, specific_person_index, func)\n",
    "        else:\n",
    "            self.specific_person_present = False\n",
    "            self._annotate_faces(faces, frame, similarities, func=None)\n",
    "\n",
    "    def _initialize_trackers(self, faces, frame, similarities, specific_person_index, func):\n",
    "        trackers = []\n",
    "        for idx, face in enumerate(faces):\n",
    "            bbox = face.bbox.astype(int)\n",
    "            name = \"Specific Person\" if idx == specific_person_index else \"Candidate\"\n",
    "            tracker = self._create_tracker(frame, bbox)\n",
    "            trackers.append(tracker)\n",
    "            self._annotate_frame(bbox, frame, name, similarities[idx], func if idx == specific_person_index else None)\n",
    "\n",
    "    def _update_trackers(self, frame, func):\n",
    "        new_trackers = []\n",
    "        new_face_names = []\n",
    "        new_face_similarities = []\n",
    "        specific_person_still_present = False\n",
    "\n",
    "        for tracker, name, similarity in zip(self.trackers, self.face_names, self.face_similarities):\n",
    "            success, tracker_bbox = tracker.update(frame)\n",
    "            if success:\n",
    "                tracker_bbox = tuple(map(int, tracker_bbox))\n",
    "                new_trackers.append(tracker)\n",
    "                new_face_names.append(name)\n",
    "                new_face_similarities.append(similarity)\n",
    "\n",
    "                # Annotate frame and apply face swap if needed\n",
    "                self.annotate_frame(tracker_bbox, frame, name, similarity, func)\n",
    "\n",
    "                if name == \"Specific Person\":\n",
    "                    specific_person_still_present = True\n",
    "            else:\n",
    "                if name == \"Specific Person\":\n",
    "                    specific_person_still_present = False\n",
    "\n",
    "        # Update trackers and face info\n",
    "        self.trackers = new_trackers\n",
    "        self.face_names = new_face_names\n",
    "        self.face_similarities = new_face_similarities\n",
    "\n",
    "        if not specific_person_still_present:\n",
    "            self.specific_person_present = False\n",
    "            self.trackers = []\n",
    "            self.face_names = []\n",
    "            self.face_similarities = []\n",
    "\n",
    "    def _calculate_similarities(self, faces):\n",
    "        return [cosine_similarity([self.known_face_embedding], [face.embedding])[0][0] for face in faces]\n",
    "\n",
    "    def _find_specific_person_index(self, similarities):\n",
    "        max_similarity = max(similarities, default=0)\n",
    "        if max_similarity > self.tolerance:\n",
    "            return similarities.index(max_similarity)\n",
    "        return None\n",
    "\n",
    "    def _display_or_save_frame(self, frame, video_writer, frame_count, total_frames):\n",
    "        cv2.putText(frame, f\"Frame: {frame_count}/{total_frames}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        if self.display_video:\n",
    "            cv2.imshow('Video', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                return\n",
    "        if video_writer:\n",
    "            video_writer.write(frame)\n",
    "\n",
    "    def _create_tracker(self, frame, bbox):\n",
    "        tracker = cv2.legacy.TrackerKCF_create()\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        tracker_bbox = (x1, y1, x2 - x1, y2 - y1)\n",
    "        tracker.init(frame, tracker_bbox)\n",
    "        return tracker\n",
    "\n",
    "    def _annotate_frame(self, bbox, frame, name, similarity, func):\n",
    "        left, top, width, height = map(int, bbox)\n",
    "        expanded_bbox = self._expand_bbox(left, top, width, height, frame.shape)\n",
    "        face_region = frame[expanded_bbox[1]:expanded_bbox[3], expanded_bbox[0]:expanded_bbox[2]]\n",
    "\n",
    "        if name == \"Specific Person\" and func:\n",
    "            swap_image = func(face_region)\n",
    "            if swap_image is not None:\n",
    "                resized_swap_image = cv2.resize(swap_image, (expanded_bbox[2] - expanded_bbox[0], expanded_bbox[3] - expanded_bbox[1]))\n",
    "                frame[expanded_bbox[1]:expanded_bbox[3], expanded_bbox[0]:expanded_bbox[2]] = resized_swap_image\n",
    "\n",
    "        color = (0, 0, 255) if name == \"Specific Person\" else (255, 0, 0) if name == \"Candidate\" else (0, 255, 0)\n",
    "        self._draw_annotation(frame, expanded_bbox, name, similarity, color)\n",
    "\n",
    "    def _expand_bbox(self, left, top, width, height, frame_shape):\n",
    "        expand_ratio = 0.3\n",
    "        expand_width = int(width * expand_ratio)\n",
    "        expand_height = int(height * expand_ratio)\n",
    "        expanded_left = max(0, left - expand_width)\n",
    "        expanded_top = max(0, top - expand_height)\n",
    "        expanded_right = min(frame_shape[1], left + width + expand_width)\n",
    "        expanded_bottom = min(frame_shape[0], top + height + expand_height)\n",
    "        return expanded_left, expanded_top, expanded_right, expanded_bottom\n",
    "\n",
    "    def _draw_annotation(self, frame, bbox, name, similarity, color):\n",
    "        cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)\n",
    "        cv2.putText(frame, name, (bbox[0], bbox[3] + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        cv2.putText(frame, f\"Similarity: {similarity:.2f}\", (bbox[0], bbox[3] + 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    def _convert_to_frame_range(self, start_time_str, duration, fps):\n",
    "        start_seconds = self._time_str_to_seconds(start_time_str)\n",
    "        end_seconds = start_seconds + duration\n",
    "        start_frame = int(start_seconds * fps)\n",
    "        end_frame = int(end_seconds * fps)\n",
    "        return start_frame, end_frame\n",
    "\n",
    "    def _time_str_to_seconds(self, time_str):\n",
    "        minutes, seconds = map(int, time_str.split(':'))\n",
    "        return minutes * 60 + seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Face Recognition Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tiffanie.kim/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tiffanie.kim/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tiffanie.kim/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tiffanie.kim/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tiffanie.kim/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    }
   ],
   "source": [
    "# Set the base image and target video\n",
    "base_image = \"test_hanni2.jpg\"\n",
    "target_video = \"hanni.mp4\"\n",
    "\n",
    "# Define the segments to process (list of tuples with start time and duration in seconds)\n",
    "segments = [(\"00:00\", 10), (\"01:00\", 15)]  # Process from 0:00 for 10 seconds, and from 1:00 for 5 seconds\n",
    "\n",
    "# Create an instance of VideoFaceSwapper with the segments\n",
    "swapper = VideoFaceSwapper(\n",
    "            base_image, \n",
    "            target_video,\n",
    "            display_video=True, \n",
    "            display_rectangle=True, \n",
    "            segments=segments,\n",
    "            )\n",
    "\n",
    "@swapper.video_swap\n",
    "def recognize_faces(face_region):\n",
    "    return None\n",
    "\n",
    "# Start the face swap process\n",
    "recognize_faces()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Face Swap Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base image and target video\n",
    "base_image = \"test_hanni2.jpg\"\n",
    "target_video = \"hanni.mp4\"\n",
    "\n",
    "# Define the segments to process (list of tuples with start time and duration in seconds)\n",
    "segments = [(\"00:00\", 10), (\"01:00\", 15)]  # Process from 0:00 for 10 seconds, and from 1:00 for 5 seconds\n",
    "\n",
    "# Create an instance of VideoFaceSwapper with the segments\n",
    "# If output_video ends with '.webm', it will be saved in WebM format.\n",
    "swapper = VideoFaceSwapper(\n",
    "            base_image, \n",
    "            target_video, \n",
    "            output_video=\"output.mp4\", \n",
    "            display_video=True, \n",
    "            display_rectangle=True, \n",
    "            segments=segments,\n",
    "            )\n",
    "\n",
    "# Create an instance of FaceSwapper\n",
    "face_swapper = FaceSwapper(det_size=(320, 320))\n",
    "\n",
    "# Set the source face\n",
    "source_image = \"./faces/kimhs.jpg\"\n",
    "success = face_swapper.set_source_face(source_image)\n",
    "if not success:\n",
    "    print(\"Failed to set source face.\")\n",
    "    exit()\n",
    "\n",
    "@swapper.video_swap\n",
    "def swap_other_face(face_region):\n",
    "    swap_image = face_swapper.swap_faces_in_image(face_region)\n",
    "    return swap_image if swap_image is not None else face_region\n",
    "\n",
    "# Start the face swap process\n",
    "swap_other_face()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
