{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [실습 환경 Setting]\n",
    "### 1. CodeFormer 설치\n",
    "- Download CodeFormer pjt :\n",
    "    ```\n",
    "    git clone https://github.com/sczhou/CodeFormer.git\n",
    "    <python_home>/Lib/site-packages 위치에 CodeFormer directory 통 copy\n",
    "    cd <python_home>/Lib/site-packages/CodeFormer\n",
    "    pip install -r requirements.txt\n",
    "    python basicsr/setup.py develop\n",
    "    ```\n",
    "### 2. CodeFormer 용 AI Model 설치\n",
    "- Download CodeFormer model(Face Restoring model):\n",
    "    https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth\n",
    "    \n",
    "    저장 위치 : python code 에 model 위치 명시\n",
    "- Download Face relevant models :\n",
    "    https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/detection_Resnet50_Final.pth\n",
    "    https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/parsing_parsenet.pth\n",
    "    \n",
    "    저장 위치 : <python_home>/Lib/site-packages/CodeFormer/weights/facelib\n",
    "\n",
    "### 3. insightface 설치\n",
    "- pip install insightface\n",
    "- pip install onnxruntime #for CPU-only\n",
    "- pip install onnxruntime-gpu #For GPU\n",
    "### 4. OpenCV 재설치\n",
    "- insightface 설치 시 opencv-python(또는 opencv-python-headless)가 자동 설치됨. uninstall 후 opencv-contrib-python 설치\n",
    "- pip uninstall opencv-python\n",
    "- pip uninstall opencv-python-headless\n",
    "- pip install opencv-contrib-python\n",
    "### 5. insightface 용 AI Model 설치\n",
    "#### 5-1. Facial Anaysis 모델 설치\n",
    "- buffalo_l download from : \n",
    "    https://github.com/deepinsight/insightface/releases\n",
    "\n",
    "- unzip buffalo_l.zip on `C:\\Users\\<user>\\.insightface\\models\\buffalo_l`\n",
    "- Facial Analysis 모델 : \n",
    "    - 얼굴 감지: 먼저 이미지에서 얼굴을 찾아내고, 얼굴의 위치 감지. 여러 얼굴이 있는 경우 각 얼굴의 위치를 정확하게 추출.\n",
    "    - 얼굴 특징 추출: 감지된 얼굴에서 고유한 특징(임베딩)을 추출하여 이를 벡터 형식으로 표현. 이 특징은 각 얼굴을 고유하게 나타내며, 다른 얼굴과 비교할 때 사용될 수 있음.\n",
    "- `buffalo_l` 모델이 insightface에서 사용할 수 있는 모델 중 정확도가 가장(그나마) 높음\n",
    "```\n",
    "Recognition Accuracy:\n",
    "\n",
    "+-------+-------+--------+-------+--------+--------+------+----+------+-------+\n",
    "| Name  | MR-ALL| African| Cauca | South  | East   | LFW  | CF | AgeD | IJB-C |\n",
    "|       |       |        | sian  | Asian  | Asian  |      | P- | B-30 | (E4)  |\n",
    "|       |       |        |       |        |        |      | FP |      |       |\n",
    "+=======+=======+========+=======+========+========+======+====+======+=======+\n",
    "| buffa | 91.25 | 90.29  | 94.70 | 93.16  | 74.96  | 99.83| 99 | 98.23| 97.25 |\n",
    "| lo_l  |       |        |       |        |        |      | .33|      |       |\n",
    "```\n",
    "#### 5-2. Face Swap 모델 설치\n",
    "- Download `inswapper_128.onnx`\n",
    "- 저장 위치 : python code 에 model 위치 명시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [실습]\n",
    "### 1. Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 얼굴 인식을 위해 InsightFace를 사용하는 샘플 코드\n",
    "\n",
    "import cv2\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# FaceAnalysis 객체 초기화 (사전 학습된 모델 사용)\n",
    "app = FaceAnalysis(name='buffalo_l')  # 'buffalo_l'는 사전 학습된 모델 이름입니다.\n",
    "app.prepare(ctx_id=-1)  # ctx_id=0은 GPU 사용, ctx_id=-1은 CPU 사용\n",
    "\n",
    "# NMS 임계값 설정\n",
    "# - 낮출수록 더 많은 얼굴이 검출될 수 있지만 오탐률이 증가할 수 있음\n",
    "app.det_model.nms_thresh = 0.6\n",
    "\n",
    "# 이미지 파일 읽기\n",
    "img = cv2.imread(\"./faces/group_image.jpg\")  # 처리할 이미지 파일의 경로로 변경하세요.\n",
    "if img is None:\n",
    "    raise FileNotFoundError(f\"이미지를 불러올 수 없습니다. 경로를 확인하세요\")\n",
    "\n",
    "# 얼굴 검출 및 임베딩 추출\n",
    "faces = app.get(img)\n",
    "\n",
    "# 검출된 얼굴 처리\n",
    "for idx, face in enumerate(faces):\n",
    "    # 얼굴 영역 표시\n",
    "    bbox = face.bbox.astype(int)\n",
    "    cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "    # 얼굴 임베딩 출력\n",
    "    #print(f\"얼굴 {idx+1} 임베딩 벡터:\\n{face.embedding}\")\n",
    "\n",
    "# 결과 이미지 표시\n",
    "cv2.imshow('Detection Result', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# FaceAnalysis 객체 초기화 (사전 학습된 모델 사용)\n",
    "app = FaceAnalysis(name='buffalo_l')  # 'buffalo_l'는 사전 학습된 모델 이름입니다.\n",
    "app.prepare(ctx_id=-1)  # ctx_id=0은 GPU 사용, ctx_id=-1은 CPU 사용\n",
    "\n",
    "# 비교할 기준 얼굴 이미지 로드 및 임베딩 추출\n",
    "ref_img_path = \"./faces/old_man.jpg\"  # 기준 얼굴 이미지 경로로 변경하세요.\n",
    "ref_img = cv2.imread(ref_img_path)\n",
    "if ref_img is None:\n",
    "    raise FileNotFoundError(f\"기준 이미지를 불러올 수 없습니다. 경로를 확인하세요: {ref_img_path}\")\n",
    "\n",
    "ref_faces = app.get(ref_img)\n",
    "if len(ref_faces) == 0:\n",
    "    raise ValueError(\"기준 이미지에서 얼굴을 검출하지 못했습니다.\")\n",
    "\n",
    "# 기준 얼굴의 임베딩 추출 (첫 번째 얼굴 사용)\n",
    "ref_embedding = ref_faces[0].embedding\n",
    "\n",
    "# 비교할 대상 이미지 로드 및 얼굴 임베딩 추출\n",
    "target_img_path = \"./faces/group_image.jpg\"  # 대상 이미지 경로로 변경하세요.\n",
    "target_img = cv2.imread(target_img_path)\n",
    "if target_img is None:\n",
    "    raise FileNotFoundError(f\"대상 이미지를 불러올 수 없습니다. 경로를 확인하세요: {target_img_path}\")\n",
    "\n",
    "target_faces = app.get(target_img)\n",
    "\n",
    "# target_faces를 x축 기준으로 정렬 (좌에서 우로)\n",
    "target_faces.sort(key=lambda face: face.bbox[0])\n",
    "\n",
    "# 검출된 얼굴들에 대해 유사도 계산 및 표시\n",
    "for idx, face in enumerate(target_faces):\n",
    "    # 대상 얼굴의 임베딩 추출\n",
    "    target_embedding = face.embedding\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    similarity = cosine_similarity([ref_embedding], [target_embedding])[0][0]\n",
    "\n",
    "    # 유사도 출력\n",
    "    print(f\"얼굴 {idx} 유사도: {similarity:.4f}\")\n",
    "\n",
    "    # 얼굴 영역 표시\n",
    "    bbox = face.bbox.astype(int)\n",
    "    cv2.rectangle(target_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "    # 유사도 텍스트 표시\n",
    "    cv2.putText(target_img, f\"{idx} : {similarity:.2f}\", (bbox[0], bbox[1]-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # 이미지 너무 커서 1/4로 줄임\n",
    "    resized_targer_img = cv2.resize(target_img, (int(target_img.shape[1] / 2), int(target_img.shape[0] / 2)))\n",
    "\n",
    "# 결과 이미지 표시\n",
    "cv2.imshow('Similarity Result', resized_targer_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Face Swapping\n",
    "#### 3-1. Face Swap 모델\n",
    "- `inswapper_128.onnx`\n",
    "- Face Swap 모델은 얼굴을 교체하는 데 중점\n",
    "- 128x128 해상도의 얼굴 이미지를 사용, 원본 얼굴 이미지의 세부 사항이 손실\n",
    "- Face Restoration 기술을 사용해 손실된 디테일을 복원하고, 저화질의 이미지를 향상 필요\n",
    "#### 3-2. Class FaceSwapper\n",
    "- `ndarray` Type image를 받아서 Face Detection 후 Face Swap 를 수행하는 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.data import get_image as ins_get_image\n",
    "import numpy as np\n",
    "\n",
    "assert insightface.__version__ >= '0.7'\n",
    "\n",
    "#SWAPPER_MODLE = 'C:\\\\Users\\\\tanmi\\\\stable-diffusion-webui\\\\models\\\\insightface\\\\inswapper_128.onnx'\n",
    "SWAPPER_MODLE = 'C:\\\\pypjt\\\\env\\\\inswapper_128.onnx'\n",
    "\n",
    "class FaceSwapper:\n",
    "    def __init__(self, model_name='buffalo_l', ctx_id=0, det_size=(640, 640)):\n",
    "        # 얼굴 분석 모델 초기화\n",
    "        self.app = FaceAnalysis(name=model_name)\n",
    "        self.app.prepare(ctx_id=ctx_id, det_size=det_size)\n",
    "        # 얼굴 교체 모델 로드\n",
    "        self.swapper = insightface.model_zoo.get_model(\n",
    "            SWAPPER_MODLE, download=True, download_zip=True\n",
    "        )\n",
    "        # 소스 얼굴 초기화\n",
    "        self.source_face = None\n",
    "        self.enhanced=False\n",
    "\n",
    "    def set_source_face(self, img, face_index=0, enhanced=False):\n",
    "        \"\"\"\n",
    "        이미지에서 소스 얼굴을 설정합니다.\n",
    "        img: 이미지 파일 경로나 numpy.ndarray 이미지\n",
    "        face_index: 선택할 얼굴의 인덱스 (기본값: 0)\n",
    "        \"\"\"\n",
    "        # 이미지 로드 (파일 경로 또는 ndarray 처리)\n",
    "        if isinstance(img, str):\n",
    "            img = cv2.imread(img)\n",
    "            if img is None:\n",
    "                print(f\"이미지를 로드할 수 없습니다: {img}\")\n",
    "                return False\n",
    "        elif not isinstance(img, np.ndarray):\n",
    "            print(\"유효한 이미지 또는 이미지 경로를 입력해 주세요.\")\n",
    "            return False\n",
    "\n",
    "        # 얼굴 검출\n",
    "        faces = self.app.get(img)\n",
    "        if len(faces) == 0:\n",
    "            print(\"소스 이미지에서 얼굴을 감지하지 못했습니다.\")\n",
    "            return False\n",
    "        # 얼굴을 x 좌표 기준으로 정렬\n",
    "        faces = sorted(faces, key=lambda x: x.bbox[0])\n",
    "        if face_index >= len(faces):\n",
    "            print(f\"소스 얼굴 인덱스가 범위를 벗어났습니다. 총 감지된 얼굴 수: {len(faces)}\")\n",
    "            return False\n",
    "        # 소스 얼굴 설정\n",
    "        self.source_face = faces[face_index]\n",
    "        print(f\"소스 얼굴이 설정되었습니다. 인덱스: {face_index}\")\n",
    "\n",
    "        self.enhanced=enhanced\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def enhance_image(self, img):\n",
    "        # 샤프닝\n",
    "        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "        sharpened = cv2.filter2D(img, -1, kernel)\n",
    "        \n",
    "        # 노이즈 제거\n",
    "        denoised = cv2.fastNlMeansDenoisingColored(sharpened, None, 10, 10, 7, 21)\n",
    "        \n",
    "        # 대비 향상\n",
    "        lab = cv2.cvtColor(denoised, cv2.COLOR_BGR2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        enhanced_lab = cv2.merge((cl,a,b))\n",
    "        enhanced = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        return enhanced\n",
    "    \n",
    "    def swap_faces_in_image(self, img):\n",
    "        \"\"\"\n",
    "        ndarray 이미지를 입력으로 받아 얼굴 교체를 수행하고, 결과 이미지를 반환합니다.\n",
    "        \"\"\"\n",
    "        if self.source_face is None:\n",
    "            print(\"소스 얼굴이 설정되지 않았습니다. 먼저 set_source_face 메서드를 호출하여 소스 얼굴을 설정하세요.\")\n",
    "            return None\n",
    "        # 얼굴 검출\n",
    "        faces = self.app.get(img)\n",
    "        if len(faces) == 0:\n",
    "            print(\"대상 이미지에서 얼굴이 감지되지 않았습니다.\")\n",
    "            return None\n",
    "        # 얼굴을 x 좌표 기준으로 정렬\n",
    "        faces = sorted(faces, key=lambda x: x.bbox[0])\n",
    "        # 얼굴 교체 수행\n",
    "        res = img.copy()\n",
    "        for face in faces:\n",
    "            res = self.swapper.get(res, face, self.source_face, paste_back=True)\n",
    "        \n",
    "        # 이미지 품질 향상\n",
    "        if self.enhanced:\n",
    "            res = self.enhance_image(res)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def extract_and_swap_faces_in_image(self, img):\n",
    "        \"\"\"\n",
    "        ndarray 이미지를 입력으로 받아 개별 얼굴을 교체한 이미지를 반환합니다.\n",
    "        \"\"\"\n",
    "        if self.source_face is None:\n",
    "            print(\"소스 얼굴이 설정되지 않았습니다. 먼저 set_source_face 메서드를 호출하여 소스 얼굴을 설정하세요.\")\n",
    "            return None\n",
    "        # 얼굴 검출\n",
    "        faces = self.app.get(img)\n",
    "        if len(faces) == 0:\n",
    "            print(\"대상 이미지에서 얼굴이 감지되지 않았습니다.\")\n",
    "            return None\n",
    "        # 얼굴을 x 좌표 기준으로 정렬\n",
    "        faces = sorted(faces, key=lambda x: x.bbox[0])\n",
    "        # 개별 얼굴 교체 및 추출\n",
    "        res = []\n",
    "        for face in faces:\n",
    "            _img, _ = self.swapper.get(img, face, self.source_face, paste_back=False)\n",
    "            res.append(_img)\n",
    "        if len(res) == 0:\n",
    "            print(\"교체된 얼굴이 없습니다.\")\n",
    "            return None\n",
    "        res = np.concatenate(res, axis=1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-3. Face Swapping example\n",
    "- 전체 얼굴 모두 Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_swapper = FaceSwapper(det_size=(320, 320))\n",
    "\n",
    "# 소스 얼굴 이미지 로드\n",
    "source_img = cv2.imread(\"./faces/asian_girl.jpg\")\n",
    "    \n",
    "# 소스 얼굴 설정 (face_index는 선택 사항)\n",
    "success = face_swapper.set_source_face(source_img, face_index=0)\n",
    "if not success:\n",
    "    print(\"소스 얼굴 설정에 실패했습니다.\")\n",
    "    exit()\n",
    "    \n",
    "# 대상 이미지 로드\n",
    "target_img = cv2.imread('./faces/group_image.jpg')\n",
    "    \n",
    "# 얼굴 교체 수행 (ndarray 이미지를 입력으로 받아 결과를 ndarray로 반환)\n",
    "swapped_img = face_swapper.swap_faces_in_image(target_img)\n",
    "\n",
    "if swapped_img is not None:\n",
    "    # 결과 이미지 표시\n",
    "    cv2.imshow('Result', swapped_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()    \n",
    "else:\n",
    "    print(\"얼굴 교체에 실패했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특정인 얼굴을 식별해서 Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# FaceAnalysis 객체 초기화 (사전 학습된 모델 사용)\n",
    "app = FaceAnalysis(name='buffalo_l')  # 'buffalo_l'는 사전 학습된 모델 이름입니다.\n",
    "app.prepare(ctx_id=-1)  # ctx_id=0은 GPU 사용, ctx_id=-1은 CPU 사용\n",
    "\n",
    "# 비교할 기준 얼굴 이미지 로드 및 임베딩 추출\n",
    "ref_img = cv2.imread(\"./faces/old_man.jpg\")\n",
    "if ref_img is None:\n",
    "    raise FileNotFoundError(f\"기준 이미지를 불러올 수 없습니다. 경로를 확인하세요: {ref_img_path}\")\n",
    "\n",
    "ref_faces = app.get(ref_img)\n",
    "if len(ref_faces) == 0:\n",
    "    raise ValueError(\"기준 이미지에서 얼굴을 검출하지 못했습니다.\")\n",
    "\n",
    "# 기준 얼굴의 임베딩 추출 (첫 번째 얼굴 사용)\n",
    "ref_embedding = ref_faces[0].embedding\n",
    "\n",
    "# 소스 얼굴 이미지 로드\n",
    "source_img = cv2.imread(\"./faces/asian_girl.jpg\")\n",
    "    \n",
    "# 소스 얼굴 설정 (face_index는 선택 사항)\n",
    "face_swapper = FaceSwapper(det_size=(320, 320))\n",
    "success = face_swapper.set_source_face(source_img, face_index=0)\n",
    "if not success:\n",
    "    print(\"소스 얼굴 설정에 실패했습니다.\")\n",
    "    exit()\n",
    "    \n",
    "# 대상 이미지 로드\n",
    "target_img = cv2.imread('./faces/group_image.jpg')\n",
    "\n",
    "target_faces = app.get(target_img)\n",
    "\n",
    "# 검출된 얼굴들에 대해 유사도 계산 및 표시\n",
    "for face in target_faces:\n",
    "    # 대상 얼굴의 임베딩 추출\n",
    "    target_embedding = face.embedding\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    similarity = cosine_similarity([ref_embedding], [target_embedding])[0][0]\n",
    "\n",
    "    # 얼굴 영역 표시\n",
    "    bbox = face.bbox.astype(int)\n",
    "\n",
    "    if similarity > 0.3:\n",
    "        # Extract face region\n",
    "        face_region = target_img[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "        swapped_img = face_swapper.swap_faces_in_image(face_region)\n",
    "        target_img[bbox[1]:bbox[3], bbox[0]:bbox[2]] = swapped_img\n",
    "\n",
    "    cv2.rectangle(target_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "    # 유사도 텍스트 표시\n",
    "    cv2.putText(target_img, f\"{similarity:.2f}\", (bbox[0], bbox[1]-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# 결과 이미지 표시\n",
    "cv2.imshow('Swap Result', target_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Face Restoration\n",
    "#### 4-1. Face Restoration 모델\n",
    "- `codeformer.pth`\n",
    "- Face Swap 후에 디테일을 복구하여 최종 이미지를 더 매끄럽고 고품질로 만드는 것이 목적\n",
    "#### 4-2. Function `restore_face`\n",
    "- image에서 얼굴을 찾아 복구(restoration) 함\n",
    "- `ndarray` type image를 받아서 `ndarray` type image로 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision.transforms.functional import normalize\n",
    "from basicsr.utils import img2tensor, tensor2img\n",
    "from facelib.utils.face_restoration_helper import FaceRestoreHelper\n",
    "from basicsr.utils.registry import ARCH_REGISTRY\n",
    "\n",
    "# 모델 경로 설정\n",
    "CODEFORMER_MODEL = \"C:\\\\pypjt\\\\env\\\\codeformer.pth\"\n",
    "\n",
    "def restore_face(input_image, use_gpu=False):\n",
    "    \"\"\"\n",
    "    얼굴 복원 함수\n",
    "    \n",
    "    :param input_image: ndarray 타입의 입력 이미지\n",
    "    :param model_path: CodeFormer 모델 파일 경로\n",
    "    :param use_gpu: GPU 사용 여부 (기본값: False)\n",
    "    :return: ndarray 타입의 복원된 이미지\n",
    "    \"\"\"\n",
    "    # 모델 로드\n",
    "    device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "    model = ARCH_REGISTRY.get('CodeFormer')(dim_embd=512, codebook_size=1024, n_head=8, n_layers=9, connect_list=['32', '64', '128', '256']).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(CODEFORMER_MODEL, weights_only=True, map_location=device)['params_ema']\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    # 이미지 크기 저장\n",
    "    h, w, _ = input_image.shape\n",
    "\n",
    "    # 얼굴 검출 및 정렬\n",
    "    face_helper = FaceRestoreHelper(\n",
    "        upscale_factor=1,\n",
    "        face_size=512,\n",
    "        crop_ratio=(1, 1),\n",
    "        det_model='retinaface_resnet50',\n",
    "        save_ext='png',\n",
    "        use_parse=True,\n",
    "        device=device\n",
    "    )\n",
    "    face_helper.read_image(input_image)\n",
    "    face_helper.get_face_landmarks_5(only_center_face=False, resize=640, eye_dist_threshold=5)\n",
    "    face_helper.align_warp_face()\n",
    "\n",
    "    # 얼굴 복원\n",
    "    for idx, cropped_face in enumerate(face_helper.cropped_faces):\n",
    "        cropped_face_t = img2tensor(cropped_face / 255., bgr2rgb=True, float32=True)\n",
    "        normalize(cropped_face_t, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n",
    "        cropped_face_t = cropped_face_t.unsqueeze(0).to(device)\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                output = model(cropped_face_t, w=0.5, adain=True)[0]\n",
    "                restored_face = tensor2img(output, rgb2bgr=True, min_max=(-1, 1))\n",
    "            del output\n",
    "            if use_gpu:\n",
    "                torch.cuda.empty_cache()\n",
    "        except RuntimeError as error:\n",
    "            print(f'Error: {error}')\n",
    "            print('If you encounter CUDA out of memory, try to set --tile with a smaller number.')\n",
    "        else:\n",
    "            restored_face = restored_face.astype('uint8')\n",
    "            face_helper.add_restored_face(restored_face)\n",
    "\n",
    "    # 결과 생성\n",
    "    face_helper.get_inverse_affine(None)\n",
    "    restored_img = face_helper.paste_faces_to_input_image()\n",
    "    \n",
    "    # 최종 이미지 크기 조정 (원본 크기로)\n",
    "    restored_img = cv2.resize(restored_img, (w, h))\n",
    "\n",
    "    return restored_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3. Face Restoring Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 입력 이미지 로드\n",
    "input_img = cv2.imread(\"./faces/asian_girl_low_quality.jpg\")\n",
    "\n",
    "# 얼굴 복원 함수 호출\n",
    "restored_img = restore_face(input_img)\n",
    "\n",
    "if restored_img is not None:\n",
    "    # 텍스트를 추가할 이미지 복사\n",
    "    input_with_text = input_img.copy()\n",
    "    restored_with_text = restored_img.copy()\n",
    "\n",
    "    # 폰트 설정\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1\n",
    "    color = (255, 255, 255)  # 흰색\n",
    "    thickness = 2\n",
    "\n",
    "    # 텍스트 추가 (변경 전)\n",
    "    cv2.putText(input_with_text, \"Before\", (10, 30), font, font_scale, color, thickness)\n",
    "\n",
    "    # 텍스트 추가 (변경 후)\n",
    "    cv2.putText(restored_with_text, \"After\", (10, 30), font, font_scale, color, thickness)\n",
    "\n",
    "    # 입력 이미지와 복원된 이미지를 좌우로 붙이기\n",
    "    combined_img = np.hstack((input_with_text, restored_with_text))\n",
    "\n",
    "    # 결과 이미지 표시\n",
    "    cv2.imshow('Result', combined_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()    \n",
    "else:\n",
    "    print(\"얼굴 복구에 실패했습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-4. Face Swap & Restoring Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# FaceSwapper 클래스 초기화\n",
    "face_swapper = FaceSwapper(det_size=(320, 320))\n",
    "\n",
    "# 소스 얼굴 이미지 로드\n",
    "source_img = cv2.imread(\"./faces/old_man.jpg\")\n",
    "\n",
    "# 소스 얼굴 설정\n",
    "success = face_swapper.set_source_face(source_img, face_index=0)\n",
    "if not success:\n",
    "    print(\"소스 얼굴 설정에 실패했습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 대상 이미지 로드\n",
    "target_img = cv2.imread(\"./faces/asian_girl.jpg\")\n",
    "\n",
    "# 얼굴 교체 수행\n",
    "swapped_img = face_swapper.swap_faces_in_image(target_img)\n",
    "restored_swapped_img = restore_face(swapped_img)\n",
    "\n",
    "if restored_swapped_img is not None:\n",
    "    # 텍스트 추가를 위한 폰트 설정\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 2\n",
    "    color = (221, 226, 255)  # \n",
    "    thickness = 4\n",
    "\n",
    "    # 원본 이미지에 텍스트 추가 (Original Image)\n",
    "    target_with_text = target_img.copy()\n",
    "    cv2.putText(target_with_text, \"Original Image\", (50, 50), font, font_scale, color, thickness)\n",
    "\n",
    "    # 교체된 이미지에 텍스트 추가 (Swapped Image)\n",
    "    swapped_with_text = swapped_img.copy()\n",
    "    cv2.putText(swapped_with_text, \"Swapped Image\", (50, 50), font, font_scale, color, thickness)\n",
    "\n",
    "    # 복원된 이미지에 텍스트 추가 (Restored Image)\n",
    "    restored_with_text = restored_swapped_img.copy()\n",
    "    cv2.putText(restored_with_text, \"Restored Image\", (50, 50), font, font_scale, color, thickness)\n",
    "\n",
    "    # 세 이미지를 가로로 결합\n",
    "    combined_img = np.hstack((target_with_text, swapped_with_text, restored_with_text))\n",
    "\n",
    "    # 결합된 이미지 너무 커서 1/4로 줄임\n",
    "    resized_combined_img = cv2.resize(combined_img, (int(combined_img.shape[1] / 2), int(combined_img.shape[0] / 2)))\n",
    "    # 결과 이미지 표시\n",
    "    cv2.imshow('Result', resized_combined_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(\"얼굴 교체에 실패했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. [응용] Video Face Swapping\n",
    "### 5-1. Class `VideoFaceProcessor`\n",
    "- Video상의 특정 인물의 Face에 대해 여러가지 조작을 수행하는 Class\n",
    "### 5-2. Class `VideoFaceProcessor` 적용된 주요 기술\n",
    "- OpenCV (cv2):\n",
    "    - OpenCV는 컴퓨터 비전 라이브러리로, 동영상이나 이미지 처리에 자주 사용. 이 코드에서는 동영상을 불러오고 처리하며, 얼굴 인식 영역을 그리기 위해 사용.\n",
    "- Face Detection & Recognition (InsightFace):\n",
    "    - 얼굴 탐지와 얼굴 임베딩 추출을 위해 사용. 얼굴 임베딩은 각 얼굴을 고유한 숫자 벡터로 표현하는 방식으로, 얼굴의 유사성을 측정하기 위해 활용.\n",
    "- Cosine Similarity (코사인 유사도):\n",
    "    - 두 얼굴 임베딩 벡터 간의 유사도를 계산. 유사도가 높을수록 얼굴이 더 비슷하다는 의미. 이를 통해 대상 얼굴이 참조 이미지와 얼마나 일치하는지 판단 가능.\n",
    "- Tracker (추적기):\n",
    "    - 특정 인물의 얼굴을 비디오에서 지속적으로 추적하는 데 사용. 여기서는 KCF 추적기를 사용하여 얼굴의 위치를 추적하며, 추적 결과에 따라 얼굴 swap을 적용.\n",
    "- video_swap method:\n",
    "    - 동영상을 프레임 단위로 처리하는 데코레이터. 사용자는 이 데코레이터를 이용해 프레임별로 얼굴에 추가 작업(예: Face Swap)을 수행. func 인자에 전달된 함수가 프레임 내 얼굴 영역에 대해 호출되며, 결과를 처리할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class VideoFaceProcessor:\n",
    "    def __init__(self, \n",
    "            base_image, \n",
    "            target_video, \n",
    "            tolerance=0.35, \n",
    "            output_video=None, \n",
    "            display_video=True, \n",
    "            display_rectangle=True, \n",
    "            segments=None,\n",
    "            ctx_id=-1,\n",
    "            ):\n",
    "\n",
    "        # Initialize FaceAnalysis object\n",
    "        self.app = FaceAnalysis(name='buffalo_l')\n",
    "        self.app.prepare(ctx_id=ctx_id)  # 0 Use GPU (set ctx_id=-1 to use CPU)\n",
    "\n",
    "        # Load the reference face image and extract embedding\n",
    "        ref_img = cv2.imread(base_image)\n",
    "        if ref_img is None:\n",
    "            raise FileNotFoundError(f\"Unable to load the reference image: {base_image}\")\n",
    "\n",
    "        ref_faces = self.app.get(ref_img)\n",
    "        if len(ref_faces) == 0:\n",
    "            raise ValueError(\"No faces detected in the reference image.\")\n",
    "\n",
    "        # Extract embedding of the reference face (use the first face)\n",
    "        self.known_face_embedding = ref_faces[0].embedding\n",
    "        self.target_video = target_video\n",
    "        self.output_video = output_video\n",
    "        self.display_video = display_video\n",
    "        self.display_rectangle = display_rectangle\n",
    "        self.tolerance = tolerance\n",
    "        self.specific_person_present = False  # Flag to indicate if Specific Person is present\n",
    "\n",
    "        # Segments to process\n",
    "        self.segments = self._prepare_segments(segments)\n",
    "        \n",
    "        # If output_video is None, do not use video saving feature\n",
    "        self.fourcc = self._get_video_codec(output_video)\n",
    "\n",
    "        self.trackers = []\n",
    "        self.face_names = []\n",
    "        self.face_similarities = []\n",
    "\n",
    "    def _get_video_codec(self, output_video):\n",
    "        if output_video is None:\n",
    "            return None\n",
    "        _, ext = os.path.splitext(output_video.lower())\n",
    "        return cv2.VideoWriter_fourcc(*'VP90') if ext == '.webm' else cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        \n",
    "    def _convert_to_frame_range(self, start_time_str, duration, fps):\n",
    "        start_seconds = self._time_str_to_seconds(start_time_str)\n",
    "        end_seconds = start_seconds + duration\n",
    "        start_frame = int(start_seconds * fps)\n",
    "        end_frame = int(end_seconds * fps)\n",
    "        return start_frame, end_frame\n",
    "        \n",
    "    def _prepare_segments(self, segments):\n",
    "        if segments is None:\n",
    "            return None\n",
    "        segment_frames = []\n",
    "        fps = self.get_video_fps()\n",
    "        for start_time_str, duration in segments:\n",
    "            start_frame, end_frame = self._convert_to_frame_range(start_time_str, duration, fps)\n",
    "            segment_frames.append((start_frame, end_frame))\n",
    "        return segment_frames\n",
    "\n",
    "    def _get_video_properties(self, video_capture):\n",
    "        fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        return fps, width, height, total_frames\n",
    "\n",
    "    def _initialize_video_writer(self, fps, width, height):\n",
    "        if self.output_video:\n",
    "            return cv2.VideoWriter(self.output_video, self.fourcc, fps, (width, height))\n",
    "        return None\n",
    "\n",
    "    def _calculate_similarities(self, faces):\n",
    "        return [cosine_similarity([self.known_face_embedding], [face.embedding])[0][0] for face in faces]\n",
    "\n",
    "    def _create_tracker(self, frame, bbox):\n",
    "        tracker = cv2.legacy.TrackerKCF_create()\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        tracker_bbox = (x1, y1, x2 - x1, y2 - y1)\n",
    "        tracker.init(frame, tracker_bbox)\n",
    "        return tracker\n",
    "\n",
    "    def video_swap(self, func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Video capture\n",
    "            video_capture = cv2.VideoCapture(self.target_video)\n",
    "\n",
    "            # Get video properties\n",
    "            fps, width, height, total_frames = self._get_video_properties(video_capture)\n",
    "\n",
    "            video_writer = self._initialize_video_writer(fps, width, height)\n",
    "\n",
    "            # Initialize variables\n",
    "            self.trackers = []\n",
    "            self.face_names = []\n",
    "            self.face_similarities = []\n",
    "            frame_skip = 24\n",
    "            frame_count = 0\n",
    "\n",
    "            # Convert segments to list of (start_frame, end_frame)\n",
    "            segment_frames = self.segments if self.segments else [(0, total_frames)]\n",
    "\n",
    "            # Process each segment\n",
    "            for start_frame, end_frame in segment_frames:\n",
    "\n",
    "                if start_frame >= total_frames:\n",
    "                    print(f\"Start frame {start_frame} exceeds total frames {total_frames}. Skipping segment.\")\n",
    "                    break\n",
    "\n",
    "                # Adjust end_frame if it exceeds total_frames\n",
    "                if end_frame > total_frames:\n",
    "                    end_frame = total_frames\n",
    "\n",
    "                # Set video capture to the start frame\n",
    "                video_capture.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "                frame_count = start_frame\n",
    "\n",
    "                while frame_count < end_frame:\n",
    "                    ret, frame = video_capture.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "\n",
    "                    frame_count += 1\n",
    "\n",
    "                    if self.specific_person_present and frame_count % frame_skip > 0:\n",
    "                        # Update trackers\n",
    "                        self._update_trackers(frame, func)\n",
    "                    else:\n",
    "                        # Create new tracers\n",
    "                        if self._process_first_tracker(frame, func) < 0:\n",
    "                            continue\n",
    "\n",
    "                    # Display current frame number / total frames at the top-left corner\n",
    "                    cv2.putText(frame, f\"Frame: {frame_count}/{total_frames}\", (10, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "                    # Output or display video\n",
    "                    if self.display_video:\n",
    "                        cv2.imshow('Video', frame)\n",
    "                        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                            break\n",
    "\n",
    "                    if video_writer:\n",
    "                        video_writer.write(frame)\n",
    "\n",
    "            # Cleanup\n",
    "            video_capture.release()\n",
    "            if video_writer:\n",
    "                video_writer.release()\n",
    "            if self.display_video:\n",
    "                cv2.destroyAllWindows()\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    def get_video_fps(self):\n",
    "        video_capture = cv2.VideoCapture(self.target_video)\n",
    "        return video_capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    def _update_trackers(self, frame, func):\n",
    "\n",
    "        new_trackers = []\n",
    "        new_face_names = []\n",
    "        new_face_similarities = []\n",
    "\n",
    "        for tracker, name, similarity in zip(self.trackers, self.face_names, self.face_similarities):\n",
    "            success, tracker_bbox = tracker.update(frame)\n",
    "            if success:\n",
    "                tracker_bbox = tuple(map(int, tracker_bbox))\n",
    "                new_trackers.append(tracker)\n",
    "                new_face_names.append(name)\n",
    "                new_face_similarities.append(similarity)\n",
    "\n",
    "                # Annotate frame and apply face swap if needed\n",
    "                self._annotate_frame(tracker_bbox, frame, name, similarity, func)\n",
    "\n",
    "            elif name == \"Specific Person\":\n",
    "                self.specific_person_present = False\n",
    "\n",
    "        # Update trackers and face info\n",
    "        self.trackers = new_trackers\n",
    "        self.face_names = new_face_names\n",
    "        self.face_similarities = new_face_similarities\n",
    "\n",
    "    def _process_first_tracker(self, frame, func)-> int:\n",
    "\n",
    "        # Attempt to detect Specific Person in every frame\n",
    "        self.trackers = []\n",
    "        self.face_names = []\n",
    "        self.face_similarities = []\n",
    "\n",
    "        # Detect faces and extract embeddings\n",
    "        faces = self.app.get(frame)                        \n",
    "        if not faces:\n",
    "            return -1  # No faces detected, skip to next frame\n",
    "\n",
    "        similarities = self._calculate_similarities(faces)\n",
    "        max_similarity = max(similarities, default=0)\n",
    "\n",
    "        # Determine if the Specific Person is detected\n",
    "        specific_person_index = similarities.index(max_similarity) if max_similarity > self.tolerance else None\n",
    "        self.specific_person_present = specific_person_index is not None\n",
    "\n",
    "        if self.specific_person_present:\n",
    "            # Initialize trackers and annotate frame for Specific Person\n",
    "            self._initialize_trackers(faces, frame, similarities, specific_person_index, func)\n",
    "        else:\n",
    "            \n",
    "            for idx_face, face in enumerate(faces):\n",
    "                bbox = face.bbox.astype(int)\n",
    "\n",
    "                name = \"Candidate\" if similarities[idx_face] > self.tolerance else \"Unknown\"\n",
    "\n",
    "                # Since we are not tracking, we do not initialize trackers\n",
    "                # Annotate frame without applying face swap\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                tracker_bbox = (x1, y1, x2 - x1, y2 - y1)\n",
    "                                \n",
    "                # Pass func=None to indicate no face swap should be applied\n",
    "                self._annotate_frame(tracker_bbox, frame, name, similarities[idx_face], func=None)\n",
    "\n",
    "        return 1\n",
    "\n",
    "    def _initialize_trackers(self, faces, frame, similarities, specific_person_index, func):\n",
    "\n",
    "        for idx, face in enumerate(faces):\n",
    "\n",
    "            bbox = face.bbox.astype(int)\n",
    "\n",
    "            if similarities[idx] > self.tolerance:                \n",
    "                name = \"Specific Person\" if idx == specific_person_index else \"Candidate\"\n",
    "            else:\n",
    "                name = \"Unknown\"\n",
    "\n",
    "            # Initialize tracker\n",
    "            tracker = self._create_tracker(frame, bbox)\n",
    "                                \n",
    "            self.trackers.append(tracker)\n",
    "            self.face_names.append(name)\n",
    "            self.face_similarities.append(similarities[idx])\n",
    "\n",
    "            # Annotate frame and apply face swap if needed\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            tracker_bbox = (x1, y1, x2 - x1, y2 - y1)\n",
    "            \n",
    "            self._annotate_frame(tracker_bbox, frame, name, similarities[idx], func)\n",
    "\n",
    "    def _annotate_frame(self, bbox, frame, name, similarity, func):\n",
    "\n",
    "        left, top, width, height = map(int, bbox)\n",
    "        expand_ratio = 0.3\n",
    "        expand_width = int(width * expand_ratio)\n",
    "        expand_height = int(height * expand_ratio)\n",
    "\n",
    "        expanded_left = max(0, left - expand_width)\n",
    "        expanded_top = max(0, top - expand_height)\n",
    "\n",
    "        frame_height, frame_width, _ = frame.shape\n",
    "        expanded_right = min(frame_width, left + width + expand_width)\n",
    "        expanded_bottom = min(frame_height, top + height + expand_height)\n",
    "\n",
    "        # Extract face region\n",
    "        face_region = frame[expanded_top:expanded_bottom, expanded_left:expanded_right]\n",
    "\n",
    "        if name == \"Specific Person\" and func:\n",
    "            # Apply face swap\n",
    "            swap_image = func(face_region)\n",
    "            # Replace the face region with the swapped image\n",
    "            if swap_image is not None:\n",
    "                resized_swap_image = cv2.resize(swap_image, (expanded_right - expanded_left, expanded_bottom - expanded_top))\n",
    "                frame[expanded_top:expanded_bottom, expanded_left:expanded_right] = resized_swap_image\n",
    "            color = (0, 0, 255)  # Red\n",
    "        elif name == \"Candidate\":\n",
    "            color = (255, 0, 0)  # Blue\n",
    "        else:\n",
    "            color = (0, 255, 0)  # Green\n",
    "\n",
    "        if self.display_rectangle:\n",
    "            # Draw rectangle and annotations\n",
    "            cv2.rectangle(frame, (expanded_left, expanded_top), (expanded_right, expanded_bottom), color, 2)\n",
    "            cv2.putText(frame, name, (expanded_left, expanded_bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "            cv2.putText(frame, f\"Similarity: {similarity:.2f}\", (expanded_left, expanded_bottom + 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    def _time_str_to_seconds(self, time_str):\n",
    "        # Convert \"mm:ss\" format to total seconds\n",
    "        minutes, seconds = map(int, time_str.split(':'))\n",
    "        total_seconds = minutes * 60 + seconds\n",
    "        return total_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3. Face Recognition Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base image and target video\n",
    "base_image = \"test_hanni2.jpg\" # 뉴진스 하니 얼굴 사진 확보 필요\n",
    "target_video = \"hanni.mp4\" # 추출 : https://www.youtube.com/watch?v=ek05M8eCk7M \n",
    "\n",
    "# Define the segments to process (list of tuples with start time and duration in seconds)\n",
    "segments = [(\"00:00\", 10), (\"01:00\", 15)]  # Process from 0:00 for 10 seconds, and from 1:00 for 15 seconds\n",
    "\n",
    "# Create an instance of VideoFaceSwapper with the segments\n",
    "bypass = VideoFaceProcessor(\n",
    "            base_image, \n",
    "            target_video,\n",
    "            display_video=True, \n",
    "            display_rectangle=True, \n",
    "            segments=segments,\n",
    "            )\n",
    "\n",
    "@bypass.video_swap\n",
    "def recognize_faces(face_region):\n",
    "    return None\n",
    "\n",
    "# Start the face swap process\n",
    "recognize_faces()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-4. Face Swap Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base image and target video\n",
    "base_image = \"test_hanni2.jpg\" # 뉴진스 하니 얼굴 사진 확보 필요\n",
    "target_video = \"hanni.mp4\" # 추출 : https://www.youtube.com/watch?v=ek05M8eCk7M \n",
    "\n",
    "# Define the segments to process (list of tuples with start time and duration in seconds)\n",
    "segments = [(\"00:00\", 10), (\"01:00\", 15)]  # Process from 0:00 for 10 seconds, and from 1:00 for 15 seconds\n",
    "\n",
    "# Create an instance of VideoFaceSwapper with the segments\n",
    "# If output_video ends with '.webm', it will be saved in WebM format.\n",
    "swapper = VideoFaceProcessor(\n",
    "            base_image, \n",
    "            target_video, \n",
    "            output_video=\"swapped_output.mp4\", \n",
    "            display_video=True, \n",
    "            display_rectangle=True, \n",
    "            segments=segments,\n",
    "            )\n",
    "\n",
    "# Create an instance of FaceSwapper\n",
    "face_swapper = FaceSwapper(det_size=(320, 320))\n",
    "\n",
    "# Set the source face\n",
    "source_image = \"./faces/old_man.jpg\"\n",
    "success = face_swapper.set_source_face(source_image)\n",
    "if not success:\n",
    "    print(\"Failed to set source face.\")\n",
    "    exit()\n",
    "\n",
    "@swapper.video_swap\n",
    "def swap(face_region):\n",
    "    swap_image = face_swapper.swap_faces_in_image(face_region)\n",
    "    return swap_image if swap_image is not None else face_region\n",
    "\n",
    "# Start the face swap&restore process\n",
    "swap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-5. Face Swap & Face Restoring Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base image and target video\n",
    "base_image = \"test_hanni2.jpg\" # 뉴진스 하니 얼굴 사진 확보 필요\n",
    "target_video = \"hanni.mp4\" # 추출 : https://www.youtube.com/watch?v=ek05M8eCk7M \n",
    "\n",
    "# Define the segments to process (list of tuples with start time and duration in seconds)\n",
    "segments = [(\"00:00\", 10), (\"01:00\", 15)]  # Process from 0:00 for 10 seconds, and from 1:00 for 15 seconds\n",
    "\n",
    "# Create an instance of VideoFaceSwapper with the segments\n",
    "# If output_video ends with '.webm', it will be saved in WebM format.\n",
    "swapper = VideoFaceProcessor(\n",
    "            base_image, \n",
    "            target_video, \n",
    "            output_video=\"restored_output.mp4\", \n",
    "            display_video=True, \n",
    "            display_rectangle=True, \n",
    "            segments=segments,\n",
    "            )\n",
    "\n",
    "# Create an instance of FaceSwapper\n",
    "face_swapper = FaceSwapper(det_size=(320, 320))\n",
    "\n",
    "# Set the source face\n",
    "source_image = \"./faces/old_man.jpg\"\n",
    "success = face_swapper.set_source_face(source_image)\n",
    "if not success:\n",
    "    print(\"Failed to set source face.\")\n",
    "    exit()\n",
    "\n",
    "@swapper.video_swap\n",
    "def swap_n_restore(face_region):\n",
    "    swap_image = face_swapper.swap_faces_in_image(face_region)\n",
    "    if swap_image is not None:\n",
    "        new_face = restore_face(swap_image)\n",
    "        if new_face is None:\n",
    "            new_face = swap_image\n",
    "    else:\n",
    "        new_face = face_region\n",
    "    return new_face\n",
    "\n",
    "# Start the face swap&restore process\n",
    "swap_n_restore()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
