{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [실습 환경 Setting]\n",
    "### 1. RealESRGAN 설치\n",
    "- Download RealESRGAN pjt :\n",
    "    ```\n",
    "    pip install basicsr\n",
    "    pip install facexlib\n",
    "    pip install gfpgan\n",
    "    git clone https://github.com/xinntao/Real-ESRGAN.git\n",
    "    <python_home>/Lib/site-packages 위치에 Real-ESRGAN directory 통 copy\n",
    "    cd <python_home>/Lib/site-packages/Real-ESRGAN\n",
    "    pip install -r requirements.txt\n",
    "    python setup.py develop\n",
    "    ```\n",
    "### 2. RealESRGAN 용 AI Model 설치\n",
    "- Download RealESRGAN model(Restoring model):\n",
    "    https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth\n",
    "    \n",
    "    저장 위치 : python code 에 model 위치 명시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTile 1/2\n",
      "\tTile 2/2\n",
      "업스케일링된 이미지가 upscaled_image.png에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from realesrgan import RealESRGANer\n",
    "\n",
    "def upscale_image(input_path, output_path, model_path, scale=2):\n",
    "    # 장치 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 모델 생성\n",
    "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=scale)\n",
    "    \n",
    "    # 업스케일러 생성 (half=False로 설정하여 FP32 사용)\n",
    "    upscaler = RealESRGANer(scale=scale, model_path=model_path, model=model, tile=400, tile_pad=10, pre_pad=0, half=False)\n",
    " \n",
    "    # 이미지 로드 (OpenCV 사용)\n",
    "    img = cv2.imread(input_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # BGR에서 RGB로 변환\n",
    "    \n",
    "    # 업스케일링 수행\n",
    "    output, _ = upscaler.enhance(img, outscale=scale)\n",
    "    \n",
    "    # 결과 저장 (OpenCV 사용)\n",
    "    output = cv2.cvtColor(output, cv2.COLOR_RGB2BGR)  # RGB에서 BGR로 변환\n",
    "    cv2.imwrite(output_path, output)\n",
    "    \n",
    "    print(f\"업스케일링된 이미지가 {output_path}에 저장되었습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = r'C:\\pypjt\\images\\hugging.jpg'  # 입력 이미지 경로\n",
    "    output_path = \"upscaled_image.png\"  # 출력 이미지 경로\n",
    "    model_path = r\"C:\\pypjt\\restore\\Lib\\site-packages\\Real-ESRGAN\\weights\\RealESRGAN_x4plus.pth\"  # Real-ESRGAN 모델 파일 경로\n",
    "    \n",
    "    upscale_image(input_path, output_path, model_path, scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from realesrgan import RealESRGANer\n",
    "from insightface.utils import face_align\n",
    "\n",
    "# RealESRGAN을 이용한 업스케일링\n",
    "model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n",
    "upsampler = RealESRGANer(\n",
    "    scale=2, model_path='weights/RealESRGAN_x4plus.pth', model=model, tile=400, tile_pad=10, pre_pad=0, half=True\n",
    ")\n",
    "\n",
    "# 이미지 불러오기\n",
    "image = cv2.imread(r'C:\\pypjt\\images\\hugging.jpg')\n",
    "\n",
    "# 리사이즈된 이미지를 업스케일링\n",
    "upscaled_image, _ = upsampler.enhance(image, outscale=2)\n",
    "\n",
    "# 결과 저장\n",
    "cv2.imwrite('upscaled_image.jpg', upscaled_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from basicsr.utils.realesrgan_utils import RealESRGANer\n",
    "from typing import Literal\n",
    "\n",
    "UPSCALE_MODEL_X2 = r\"C:\\pypjt\\restore\\Lib\\site-packages\\Real-ESRGAN\\weights\\RealESRGAN_x2plus.pth\"\n",
    "UPSCALE_MODEL_X4 = r\"C:\\pypjt\\restore\\Lib\\site-packages\\Real-ESRGAN\\weights\\RealESRGAN_x4plus.pth\"\n",
    "\n",
    "def upscale_image(input_image: np.ndarray, scale: Literal[2, 4] = 2) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    이미지를 업스케일링하는 함수.\n",
    "    \n",
    "    :param input_image: ndarray 타입의 입력 이미지\n",
    "    :param model_path: 모델 파일 경로\n",
    "    :param scale: 업스케일 배율 (기본값: 2)\n",
    "    :return: 업스케일링된 ndarray 타입의 이미지\n",
    "    \"\"\"\n",
    "    # 장치 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    # Upscaing Model 선택\n",
    "    model_path = UPSCALE_MODEL_X2 if scale == 2 else UPSCALE_MODEL_X4\n",
    "\n",
    "    # 모델 생성\n",
    "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=scale)\n",
    "    \n",
    "    # 업스케일러 생성 (half=False로 설정하여 FP32 사용)\n",
    "    upscaler = RealESRGANer(scale=scale, model_path=model_path, model=model, tile=400, tile_pad=10, pre_pad=0, half=False)\n",
    "    \n",
    "    # 입력 이미지가 BGR 형식일 경우, RGB로 변환\n",
    "    if input_image.shape[-1] == 3:  # 이미지가 컬러일 경우\n",
    "        input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 업스케일링 수행\n",
    "    output, _ = upscaler.enhance(input_image, outscale=scale)\n",
    "    \n",
    "    # 결과를 BGR로 다시 변환하여 반환\n",
    "    output = cv2.cvtColor(output, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\pypjt\\face\\Lib\\site-packages\\codeformer\\basicsr\\utils\\realesrgan_utils.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loadnet = torch.load(model_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 입력 이미지 로드\n",
    "input_img = cv2.imread(\"restored_man_n_waman.jpg\")\n",
    "\n",
    "# upscaling 함수 호출\n",
    "upscaled_img = upscale_image(input_img, scale=4)\n",
    "\n",
    "# 결과 저장\n",
    "cv2.imwrite('upscaled_man_n_waman2.jpg', upscaled_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision.transforms.functional import normalize\n",
    "from basicsr.utils import img2tensor, tensor2img\n",
    "from facelib.utils.face_restoration_helper import FaceRestoreHelper\n",
    "from basicsr.utils.registry import ARCH_REGISTRY\n",
    "\n",
    "# 모델 경로 설정\n",
    "CODEFORMER_MODEL = \"C:\\\\pypjt\\\\env\\\\codeformer.pth\"\n",
    "\n",
    "def restore_face(input_image, use_gpu=False):\n",
    "    \"\"\"\n",
    "    얼굴 복원 함수\n",
    "    \n",
    "    :param input_image: ndarray 타입의 입력 이미지\n",
    "    :param model_path: CodeFormer 모델 파일 경로\n",
    "    :param use_gpu: GPU 사용 여부 (기본값: False)\n",
    "    :return: ndarray 타입의 복원된 이미지\n",
    "    \"\"\"\n",
    "    # 모델 로드\n",
    "    device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "    model = ARCH_REGISTRY.get('CodeFormer')(dim_embd=512, codebook_size=1024, n_head=8, n_layers=9, connect_list=['32', '64', '128', '256']).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(CODEFORMER_MODEL, weights_only=True, map_location=device)['params_ema']\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    # 이미지 크기 저장\n",
    "    h, w, _ = input_image.shape\n",
    "\n",
    "    # 얼굴 검출 및 정렬\n",
    "    face_helper = FaceRestoreHelper(\n",
    "        upscale_factor=1,\n",
    "        face_size=512,\n",
    "        crop_ratio=(1, 1),\n",
    "        det_model='retinaface_resnet50',\n",
    "        save_ext='png',\n",
    "        use_parse=True,\n",
    "        device=device\n",
    "    )\n",
    "    face_helper.read_image(input_image)\n",
    "    face_helper.get_face_landmarks_5(only_center_face=False, resize=640, eye_dist_threshold=5)\n",
    "    face_helper.align_warp_face()\n",
    "\n",
    "    # 얼굴 복원\n",
    "    for idx, cropped_face in enumerate(face_helper.cropped_faces):\n",
    "        cropped_face_t = img2tensor(cropped_face / 255., bgr2rgb=True, float32=True)\n",
    "        normalize(cropped_face_t, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n",
    "        cropped_face_t = cropped_face_t.unsqueeze(0).to(device)\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                output = model(cropped_face_t, w=0.5, adain=True)[0]\n",
    "                restored_face = tensor2img(output, rgb2bgr=True, min_max=(-1, 1))\n",
    "            del output\n",
    "            if use_gpu:\n",
    "                torch.cuda.empty_cache()\n",
    "        except RuntimeError as error:\n",
    "            print(f'Error: {error}')\n",
    "            print('If you encounter CUDA out of memory, try to set --tile with a smaller number.')\n",
    "        else:\n",
    "            restored_face = restored_face.astype('uint8')\n",
    "            face_helper.add_restored_face(restored_face)\n",
    "\n",
    "    # 결과 생성\n",
    "    face_helper.get_inverse_affine(None)\n",
    "    restored_img = face_helper.paste_faces_to_input_image()\n",
    "    \n",
    "    # 최종 이미지 크기 조정 (원본 크기로)\n",
    "    restored_img = cv2.resize(restored_img, (w, h))\n",
    "\n",
    "    return restored_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 입력 이미지 로드\n",
    "input_img = cv2.imread(\"./images/man_n_waman.jpg\")\n",
    "\n",
    "# upscaling 함수 호출\n",
    "upscaled_img = restore_face(input_img)\n",
    "\n",
    "# 결과 저장\n",
    "cv2.imwrite('restored_man_n_waman.jpg', upscaled_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
