{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAW PITCH ROLL\n",
    "\n",
    "![Link](./images/yaw_pitch_roll.jpg)\n",
    "\n",
    "- 주의 : Pitch의 절대값이 커질수로 Roll 값의 신뢰도는 떨어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from insightface.app.common import Face\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "def get_yaw_pitch_roll(face: Face) -> Tuple[Optional[float], Optional[float], Optional[float]]:\n",
    "    return face.pose[1], face.pose[0], face.pose[2] #yaw, pitch, roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_IMG_FILE = \"cross.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 얼굴 인식을 위해 InsightFace를 사용하는 샘플 코드\n",
    "\n",
    "import cv2\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# FaceAnalysis 객체 초기화 (사전 학습된 모델 사용)\n",
    "app = FaceAnalysis(name='buffalo_l')  # 'buffalo_l'는 사전 학습된 모델 이름입니다.\n",
    "app.prepare(ctx_id=-1)  # ctx_id=0은 GPU 사용, ctx_id=-1은 CPU 사용\n",
    "\n",
    "# NMS 임계값 설정\n",
    "# - 낮출수록 더 많은 얼굴이 검출될 수 있지만 오탐률이 증가할 수 있음\n",
    "app.det_model.nms_thresh = 0.6\n",
    "\n",
    "# 이미지 파일 읽기\n",
    "#img = cv2.imread(\"./faces/group_image.jpg\")  # 처리할 이미지 파일의 경로로 변경하세요.\n",
    "#img = cv2.imread(\"full-face-view.png\")\n",
    "#img = cv2.imread(\"F1.large.jpg\")\n",
    "img = cv2.imread(TARGET_IMG_FILE)\n",
    "if img is None:\n",
    "    raise FileNotFoundError(f\"이미지를 불러올 수 없습니다. 경로를 확인하세요\")\n",
    "\n",
    "# 얼굴 검출 및 임베딩 추출\n",
    "faces = app.get(img)\n",
    "\n",
    "# 검출된 얼굴 처리\n",
    "for idx, face in enumerate(faces):\n",
    "    \n",
    "    # 얼굴 영역 표시\n",
    "    bbox = face.bbox.astype(int)\n",
    "    #cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "    # 얼굴 임베딩 출력\n",
    "    #print(f\"얼굴 {idx+1} 임베딩 벡터:\\n{face.embedding}\")\n",
    "\n",
    "    # YAW, PITCH, ROLL 계산\n",
    "    yaw, pitch, roll = get_yaw_pitch_roll(face)\n",
    "\n",
    "    # YAW, PITCH, ROLL 값 표시\n",
    "    if yaw is not None and pitch is not None and roll is not None:\n",
    "        cv2.putText(img, f\"YAW: {yaw:.1f}\", (bbox[0] + 5, bbox[1] + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (127, 127, 127), 1)\n",
    "        cv2.putText(img, f\"PITCH: {pitch:.1f}\", (bbox[0] + 5, bbox[1] + 35), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (127, 127, 127), 1)\n",
    "        cv2.putText(img, f\"ROLL: {roll:.1f}\", (bbox[0] + 5, bbox[1] + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (127, 127, 127), 1)\n",
    "\n",
    "# 결과 이미지 표시\n",
    "#cv2.imshow('Detection Result', img)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "# 결과 저장\n",
    "cv2.imwrite('detection_crooss.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import FaceSwapper, restore_face, upscale_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지 전체 Upscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(TARGET_IMG_FILE)\n",
    "# upscaling 함수 호출\n",
    "upscaled_img = upscale_image(img, scale=4)\n",
    "\n",
    "# 결과 저장\n",
    "cv2.imwrite('upscaled_crooss.jpg', upscaled_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전체 얼굴 모두 Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "face_swapper = FaceSwapper(det_size=(640, 640), nms_thresh=0.3)\n",
    "\n",
    "# 소스 얼굴 이미지 로드\n",
    "source_img = cv2.imread(\"test_hanni2.jpg\")\n",
    "    \n",
    "# 소스 얼굴 설정 (face_index는 선택 사항)\n",
    "success = face_swapper.set_source_face(source_img, face_index=0)\n",
    "if not success:\n",
    "    print(\"소스 얼굴 설정에 실패했습니다.\")\n",
    "    raise StopIteration  # 셀 실행을 중단\n",
    "    \n",
    "# 대상 이미지 로드\n",
    "target_img = cv2.imread(\"upscaled_crooss.jpg\")\n",
    "    \n",
    "# 얼굴 교체 수행 (ndarray 이미지를 입력으로 받아 결과를 ndarray로 반환)\n",
    "swapped_img = face_swapper.swap_faces_in_image(target_img, draw_rectangle=True)\n",
    "\n",
    "if swapped_img is not None:\n",
    "    # 결과 이미지 표시\n",
    "    #cv2.imshow('Result', swapped_img)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()    \n",
    "\n",
    "    # 결과 저장\n",
    "    cv2.imwrite('swapped_crooss.jpg', swapped_img)\n",
    "else:\n",
    "    print(\"얼굴 교체에 실패했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전체 얼굴 모두 Restoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# 대상 이미지 로드\n",
    "target_img = cv2.imread(\"swapped_crooss.jpg\")\n",
    "    \n",
    "# 얼굴 교체 수행 (ndarray 이미지를 입력으로 받아 결과를 ndarray로 반환)\n",
    "restored_img = restore_face(target_img, draw_rectangle=True)\n",
    "\n",
    "if restored_img is not None:\n",
    "    # 결과 이미지 표시\n",
    "    #cv2.imshow('Result', restored_img)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "\n",
    "    # 결과 저장\n",
    "    cv2.imwrite('restored_crooss.jpg', restored_img)\n",
    "else:\n",
    "    print(\"얼굴 복구에 실패했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C:\\pypjt\\face\\Lib\\site-packages\\GFPGAN\\experiments\\pretrained_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from gfpgan import GFPGANer\n",
    "\n",
    "def restore_face_gfpgan(input_image: np.ndarray, upscale: int = 2, version: str = '1.4', use_gpu: bool = False, draw_rectangle: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    GFPGAN을 사용하여 얼굴을 복원하는 함수\n",
    "\n",
    "    :param input_image: ndarray 타입의 입력 이미지\n",
    "    :param upscale: 업스케일링 배율 (기본값: 2)\n",
    "    :param version: GFPGAN 모델 버전 ('1.3' 또는 '1.4')\n",
    "    :param use_gpu: GPU 사용 여부 (기본값: False)\n",
    "    :param draw_rectangle: 복원된 얼굴에 사각형을 그릴지 여부 (기본값: True)\n",
    "    :return: ndarray 타입의 복원된 이미지\n",
    "    \"\"\"\n",
    "    # 디바이스 설정\n",
    "    device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 모델 파일 경로 설정\n",
    "    if version == '1.4':\n",
    "        model_path = './gfpgan/weights/GFPGANv1.4.pth'\n",
    "    elif version == '1.3':\n",
    "        model_path = 'experiments/pretrained_models/GFPGANv1.3.pth'\n",
    "    else:\n",
    "        raise ValueError('Invalid GFPGAN version. Choose \"1.3\" or \"1.4\".')\n",
    "\n",
    "    # GFPGAN 초기화\n",
    "    restorer = GFPGANer(\n",
    "        model_path=model_path,\n",
    "        upscale=upscale,\n",
    "        arch='clean',\n",
    "        channel_multiplier=2,\n",
    "        bg_upsampler=None,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 얼굴 복원\n",
    "    cropped_faces, restored_faces, restored_img = restorer.enhance(\n",
    "        input_image,\n",
    "        has_aligned=False,\n",
    "        only_center_face=False,\n",
    "        paste_back=True\n",
    "    )\n",
    "\n",
    "    # 복원된 얼굴에 사각형 그리기\n",
    "    if draw_rectangle:\n",
    "        for face in restorer.face_helper.det_faces:\n",
    "            x1, y1, x2, y2, _ = face.astype(int)\n",
    "            cv2.rectangle(restored_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    return restored_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\pypjt\\face\\Lib\\site-packages\\facexlib\\parsing\\__init__.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  load_net = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
      "c:\\pypjt\\face\\Lib\\site-packages\\gfpgan\\gfpgan\\utils.py:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loadnet = torch.load(model_path)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'experiments/pretrained_models/GFPGANv1.4.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m input_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswapped_crooss.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 얼굴 복원\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m restored_image \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_face_gfpgan\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdraw_rectangle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n\u001b[0;32m     10\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgfpgan_cross.png\u001b[39m\u001b[38;5;124m'\u001b[39m, restored_image)\n",
      "Cell \u001b[1;32mIn[2], line 29\u001b[0m, in \u001b[0;36mrestore_face_gfpgan\u001b[1;34m(input_image, upscale, version, use_gpu, draw_rectangle)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid GFPGAN version. Choose \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.3\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.4\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# GFPGAN 초기화\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m restorer \u001b[38;5;241m=\u001b[39m \u001b[43mGFPGANer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43march\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannel_multiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbg_upsampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 얼굴 복원\u001b[39;00m\n\u001b[0;32m     39\u001b[0m cropped_faces, restored_faces, restored_img \u001b[38;5;241m=\u001b[39m restorer\u001b[38;5;241m.\u001b[39menhance(\n\u001b[0;32m     40\u001b[0m     input_image,\n\u001b[0;32m     41\u001b[0m     has_aligned\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     42\u001b[0m     only_center_face\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     43\u001b[0m     paste_back\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     44\u001b[0m )\n",
      "File \u001b[1;32mc:\\pypjt\\face\\Lib\\site-packages\\gfpgan\\gfpgan\\utils.py:92\u001b[0m, in \u001b[0;36mGFPGANer.__init__\u001b[1;34m(self, model_path, upscale, arch, channel_multiplier, bg_upsampler, device)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     90\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m load_file_from_url(\n\u001b[0;32m     91\u001b[0m         url\u001b[38;5;241m=\u001b[39mmodel_path, model_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ROOT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgfpgan/weights\u001b[39m\u001b[38;5;124m'\u001b[39m), progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 92\u001b[0m loadnet \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams_ema\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m loadnet:\n\u001b[0;32m     94\u001b[0m     keyname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams_ema\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\pypjt\\face\\Lib\\site-packages\\torch\\serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\pypjt\\face\\Lib\\site-packages\\torch\\serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\pypjt\\face\\Lib\\site-packages\\torch\\serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'experiments/pretrained_models/GFPGANv1.4.pth'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# 이미지 읽기\n",
    "input_image = cv2.imread(\"swapped_crooss.jpg\")\n",
    "\n",
    "# 얼굴 복원\n",
    "restored_image = restore_face_gfpgan(input_image, upscale=1, use_gpu=True, draw_rectangle=True)\n",
    "\n",
    "# 결과 저장\n",
    "cv2.imwrite('gfpgan_cross.png', restored_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
