{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### llamacpp python 설치\n",
    "- MS C++ Build Tools 설치\n",
    "  https://visualstudio.microsoft.com/ko/visual-cpp-build-tools/\n",
    "- 설치 관리자를 실행하고 \"Desktop development with C++\" 워크로드를 선택하여 설치\n",
    "- pip install llama-cpp-python\n",
    "\n",
    "#### 출처\n",
    "https://python.langchain.com/v0.2/docs/integrations/llms/llamacpp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 32 key-value pairs and 258 tensors from ./aya-23-8B-Q2_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = command-r\n",
      "llama_model_loader: - kv   1:                               general.name str              = aya-23-8B\n",
      "llama_model_loader: - kv   2:                      command-r.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                   command-r.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                 command-r.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:              command-r.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:             command-r.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:          command-r.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                   command-r.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv   9:     command-r.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                      command-r.logit_scale f32              = 0.062500\n",
      "llama_model_loader: - kv  12:                command-r.rope.scaling.type str              = none\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = command-r\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<PAD>\", \"<UNK>\", \"<CLS>\", \"<SEP>\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,253333]  = [\"Ġ Ġ\", \"Ġ t\", \"e r\", \"i n\", \"Ġ a...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 5\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 255001\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:           tokenizer.chat_template.tool_use str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  24:                tokenizer.chat_template.rag str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  25:                   tokenizer.chat_templates arr[str,2]       = [\"tool_use\", \"rag\"]\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  28:                      quantize.imatrix.file str              = /models/aya-23-8B-GGUF/aya-23-8B.imatrix\n",
      "llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = /training_data/calibration_data.txt\n",
      "llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 194\n",
      "llama_model_loader: - type  f32:   33 tensors\n",
      "llama_model_loader: - type q2_K:  128 tensors\n",
      "llama_model_loader: - type q3_K:   64 tensors\n",
      "llama_model_loader: - type q4_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 1008\n",
      "llm_load_vocab: token to piece cache size = 1.8528 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = command-r\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 253333\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 6.2e-02\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = none\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (3.42 BPW) \n",
      "llm_load_print_meta: general.name     = aya-23-8B\n",
      "llm_load_print_meta: BOS token        = 5 '<BOS_TOKEN>'\n",
      "llm_load_print_meta: EOS token        = 255001 '<|END_OF_TURN_TOKEN|>'\n",
      "llm_load_print_meta: PAD token        = 0 '<PAD>'\n",
      "llm_load_print_meta: LF token         = 136 'Ä'\n",
      "llm_load_print_meta: max token length = 1024\n",
      "llm_load_tensors: ggml ctx size =    0.12 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3268.83 MiB\n",
      "..............................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    31.75 MiB\n",
      "llama_new_context_with_model: graph nodes  = 968\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'command-r.feed_forward_length': '14336', 'general.name': 'aya-23-8B', 'command-r.rope.scaling.type': 'none', 'general.architecture': 'command-r', 'command-r.block_count': '32', 'command-r.attention.head_count_kv': '8', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = 'You are Command-R, a brilliant, sophisticated, AI-assistant trained to assist human users by providing thorough responses. You are trained by Cohere.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + system_message + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}\", 'command-r.context_length': '8192', 'command-r.embedding_length': '4096', 'command-r.attention.head_count': '32', 'tokenizer.chat_template.rag': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = '## Task and Context\\\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\\\'s needs as best you can, which will be wide-ranging.\\\\n\\\\n## Style Guide\\\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.' %}{% endif %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' }}{{ '# Safety Preamble' }}{{ '\\nThe instructions in this section override those in the task description and style guide sections. Don\\\\'t answer questions that are harmful or immoral.' }}{{ '\\n\\n# System Preamble' }}{{ '\\n## Basic Rules' }}{{ '\\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\\\'s requests, you cite your sources in your answers, according to those instructions.' }}{{ '\\n\\n# User Preamble' }}{{ '\\n' + system_message }}{{ '<|END_OF_TURN_TOKEN|>'}}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'system' %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'}}{{ '<results>' }}{% for document in documents %}{{ '\\nDocument: ' }}{{ loop.index0 }}\\n{% for key, value in document.items() %}{{ key }}: {{value}}\\n{% endfor %}{% endfor %}{{ '</results>'}}{{ '<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' }}{{ 'Carefully perform the following instructions, in order, starting each with a new line.\\n' }}{{ 'Firstly, Decide which of the retrieved documents are relevant to the user\\\\'s last input by writing \\\\'Relevant Documents:\\\\' followed by comma-separated list of document numbers. If none are relevant, you should instead write \\\\'None\\\\'.\\n' }}{{ 'Secondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user\\\\'s last input by writing \\\\'Cited Documents:\\\\' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write \\\\'None\\\\'.\\n' }}{% if citation_mode=='accurate' %}{{ 'Thirdly, Write \\\\'Answer:\\\\' followed by a response to the user\\\\'s last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.\\n' }}{% endif %}{{ 'Finally, Write \\\\'Grounded answer:\\\\' followed by a response to the user\\\\'s last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.' }}{{ '<|END_OF_TURN_TOKEN|>' }}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}\", 'command-r.rope.freq_base': '10000.000000', 'command-r.attention.layer_norm_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '255001', 'general.file_type': '10', 'command-r.logit_scale': '0.062500', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'command-r', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '5', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.chat_template.tool_use': '{{ bos_token }}{% if messages[0][\\'role\\'] == \\'system\\' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][\\'content\\'] %}{% else %}{% set loop_messages = messages %}{% set system_message = \\'## Task and Context\\\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\\\\\'s needs as best you can, which will be wide-ranging.\\\\n\\\\n## Style Guide\\\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\\' %}{% endif %}{{ \\'<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\\' }}{{ \\'# Safety Preamble\\' }}{{ \\'\\nThe instructions in this section override those in the task description and style guide sections. Don\\\\\\'t answer questions that are harmful or immoral.\\' }}{{ \\'\\n\\n# System Preamble\\' }}{{ \\'\\n## Basic Rules\\' }}{{ \\'\\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\\\\\'s requests, you cite your sources in your answers, according to those instructions.\\' }}{{ \\'\\n\\n# User Preamble\\' }}{{ \\'\\n\\' + system_message }}{{\\'\\n\\n## Available Tools\\nHere is a list of tools that you have available to you:\\n\\n\\'}}{% for tool in tools %}{% if loop.index0 != 0 %}{{ \\'\\n\\n\\'}}{% endif %}{{\\'```python\\ndef \\' + tool.name + \\'(\\'}}{% for param_name, param_fields in tool.parameter_definitions.items() %}{% if loop.index0 != 0 %}{{ \\', \\'}}{% endif %}{{param_name}}: {% if not param_fields.required %}{{\\'Optional[\\' + param_fields.type + \\'] = None\\'}}{% else %}{{ param_fields.type }}{% endif %}{% endfor %}{{ \\') -> List[Dict]:\\n    \"\"\"\\'}}{{ tool.description }}{% if tool.parameter_definitions|length != 0 %}{{ \\'\\n\\n    Args:\\n        \\'}}{% for param_name, param_fields in tool.parameter_definitions.items() %}{% if loop.index0 != 0 %}{{ \\'\\n        \\' }}{% endif %}{{ param_name + \\' (\\'}}{% if not param_fields.required %}{{\\'Optional[\\' + param_fields.type + \\']\\'}}{% else %}{{ param_fields.type }}{% endif %}{{ \\'): \\' + param_fields.description }}{% endfor %}{% endif %}{{ \\'\\n    \"\"\"\\n    pass\\n```\\' }}{% endfor %}{{ \\'<|END_OF_TURN_TOKEN|>\\'}}{% for message in loop_messages %}{% set content = message[\\'content\\'] %}{% if message[\\'role\\'] == \\'user\\' %}{{ \\'<|START_OF_TURN_TOKEN|><|USER_TOKEN|>\\' + content.strip() + \\'<|END_OF_TURN_TOKEN|>\\' }}{% elif message[\\'role\\'] == \\'system\\' %}{{ \\'<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\\' + content.strip() + \\'<|END_OF_TURN_TOKEN|>\\' }}{% elif message[\\'role\\'] == \\'assistant\\' %}{{ \\'<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\\'  + content.strip() + \\'<|END_OF_TURN_TOKEN|>\\' }}{% endif %}{% endfor %}{{\\'<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Write \\\\\\'Action:\\\\\\' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user\\\\\\'s last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:\\n```json\\n[\\n    {\\n        \"tool_name\": title of the tool in the specification,\\n        \"parameters\": a dict of parameters to input into the tool as they are defined in the specs, or {} if it takes no parameters\\n    }\\n]```<|END_OF_TURN_TOKEN|>\\'}}{% if add_generation_prompt %}{{ \\'<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\\' }}{% endif %}', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.chunks_count': '194', 'quantize.imatrix.file': '/models/aya-23-8B-GGUF/aya-23-8B.imatrix', 'quantize.imatrix.dataset': '/training_data/calibration_data.txt', 'quantize.imatrix.entries_count': '224'}\n",
      "Available chat formats from metadata: chat_template.rag, chat_template.tool_use, chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = 'You are Command-R, a brilliant, sophisticated, AI-assistant trained to assist human users by providing thorough responses. You are trained by Cohere.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + system_message + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}\n",
      "Using chat eos_token: <|END_OF_TURN_TOKEN|>\n",
      "Using chat bos_token: <BOS_TOKEN>\n"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./aya-23-8B-Q2_K.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: \n",
      "The Korean Peninsula is a peninsula located in the eastern part of Asia. It is connected to mainland China, which lies east of it by land. The South Korean government is located at Seoul, while the North Korean government is based in Pyongyang. The two countries share the Korean name, but are technically considered as two separate states. \n",
      "The Korean Peninsula has been inhabited since at least 12000 BC and was one of the first regions to be settled by humans. In ancient times it was a part of the Chinese Empire, until its independence in 1895 under the government of Korea. The peninsula was invaded by Japan in 1910, but it was liberated by the Allies after World War II. \n",
      "The Korean Peninsula is known as the \"Hermit Kingdom\" because it has been closed to Western visitors for almost 30 years. It is one of the world's most isolated countries due to its political and military isolation policies. This isolation is often referred to as a \"fire wall\", because Korea is considered \"a fortress\" by its government.\n",
      "The Korean Peninsula has a population of approximately 76 million people, with about 75% of them living in South Korea. The capital of North Korea is Pyongyang, and the capital of South Korea is Seoul. Both countries are known as the Korean Peninsula or the Korean Peninsula."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1609.70 ms\n",
      "llama_print_timings:      sample time =     305.42 ms /   284 runs   (    1.08 ms per token,   929.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1609.65 ms /     8 tokens (  201.21 ms per token,     4.97 tokens per second)\n",
      "llama_print_timings:        eval time =   32710.44 ms /   283 runs   (  115.58 ms per token,     8.65 tokens per second)\n",
      "llama_print_timings:       total time =   35566.09 ms /   291 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: \\nThe Korean Peninsula is a peninsula located in the eastern part of Asia. It is connected to mainland China, which lies east of it by land. The South Korean government is located at Seoul, while the North Korean government is based in Pyongyang. The two countries share the Korean name, but are technically considered as two separate states. \\nThe Korean Peninsula has been inhabited since at least 12000 BC and was one of the first regions to be settled by humans. In ancient times it was a part of the Chinese Empire, until its independence in 1895 under the government of Korea. The peninsula was invaded by Japan in 1910, but it was liberated by the Allies after World War II. \\nThe Korean Peninsula is known as the \"Hermit Kingdom\" because it has been closed to Western visitors for almost 30 years. It is one of the world\\'s most isolated countries due to its political and military isolation policies. This isolation is often referred to as a \"fire wall\", because Korea is considered \"a fortress\" by its government.\\nThe Korean Peninsula has a population of approximately 76 million people, with about 75% of them living in South Korea. The capital of North Korea is Pyongyang, and the capital of South Korea is Seoul. Both countries are known as the Korean Peninsula or the Korean Peninsula.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Question: Describe Korea\n",
    "\"\"\"\n",
    "chain.invoke({\"question\":question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
