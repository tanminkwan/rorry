{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지에서 여성은 작은 규모의 딱딱빛을 가진 자국을 들어올리는 모습으로, 인간과 함께한 세계를 보여주고 있다.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "res = ollama.chat(\n",
    "\tmodel=\"llava:7b-v1.5-q8_0\",\n",
    "\tmessages=[\n",
    "\t\t{\n",
    "\t\t\t'role': 'user',\n",
    "\t\t\t'content': '이미지를 간결하게 묘사하세요',\n",
    "\t\t\t'images': ['./Marina_Mogilko.jpg']\n",
    "\t\t}\n",
    "\t]\n",
    ")\n",
    "\n",
    "print(res['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        buffer = BytesIO()\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "# Ollama llava 모델 초기화\n",
    "llava = Ollama(model=\"llava:7b-v1.5-q8_0\")\n",
    "\n",
    "# 이미지 경로 설정\n",
    "image_path = './Marina_Mogilko.jpg'  # 실제 이미지 경로로 변경해주세요\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "# 시스템 메시지와 프롬프트 템플릿 설정\n",
    "system_message = SystemMessage(content=\"You are an AI assistant capable of analyzing images and answering questions about them.\")\n",
    "human_template = \"Here's an image: <image>{image_data}</image>\\n\\nQuestion: {question}\\n\\nPlease analyze the image and answer the question.\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "# LLMChain 설정\n",
    "chain = LLMChain(llm=llava, prompt=chat_prompt)\n",
    "\n",
    "# 질문 리스트\n",
    "question = \"Describe the image\"\n",
    "\n",
    "response = chain.run(image_data=base64_image, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The image depicts a white cartoon bear with pink lips, holding its hands out and smiling brightly. It appears to be the main focus of the image, showcasing its innocence and cheerful nature. There are no other objects or characters in the scene.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "res = ollama.chat(\n",
    "\tmodel=\"llava:7b-v1.5-q8_0\",\n",
    "\tmessages=[\n",
    "\t\t{\n",
    "\t\t\t'role': 'user',\n",
    "\t\t\t'content': 'Describe the image',\n",
    "\t\t\t'images': ['./hello.jpg']\n",
    "\t\t}\n",
    "\t]\n",
    ")\n",
    "\n",
    "print(res['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def convert_to_base64(pil_image):\n",
    "    \"\"\"\n",
    "    Convert PIL images to Base64 encoded strings\n",
    "\n",
    "    :param pil_image: PIL image\n",
    "    :return: Re-sized Base64 string\n",
    "    \"\"\"\n",
    "\n",
    "    buffered = BytesIO()\n",
    "    pil_image.save(buffered, format=\"JPEG\")  # You can change the format if needed\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str\n",
    "\n",
    "def process_image(image_file):\n",
    "    print(f\"\\nProcessing {image_file}\\n\")\n",
    "    with Image.open(image_file) as img:\n",
    "        with BytesIO() as buffer:\n",
    "            img.save(buffer, format='JPEG')\n",
    "            image_bytes = buffer.getvalue()\n",
    "\n",
    "    return image_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ./Marina_Mogilko.jpg\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. A person is standing in a room with their hand on a table, possibly at a party or gathering.\\n2. Another person can be seen sitting near the table, perhaps engaged in conversation or simply relaxing.\\n3. There are several chairs placed around the room, indicating that it might be a dining or living area.\\n4. A bowl is also visible on the table, suggesting that food and drinks may have been served during the event.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llava:7b-v1.5-q8_0\"\n",
    ")  # assuming you have Ollama installed and have llama3 model pulled with `ollama pull llama3 `\n",
    "\n",
    "file_path = './Marina_Mogilko.jpg'\n",
    "#pil_image = Image.open(file_path)\n",
    "#image = convert_to_base64(pil_image)\n",
    "image_bytes = process_image(file_path)\n",
    "\n",
    "llm.bind(image=image_bytes)\n",
    "\n",
    "response = llm(\"Describe this image\")\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
