{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import download_youtube\n",
    "video_id = 'ek05M8eCk7M'\n",
    "#video_id = 'I-cxigjLG0I'\n",
    "youtube_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "job_id = 'Hanni'\n",
    "file_path, file_name =  download_youtube(youtube_url, job_id=job_id)\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import slice_video\n",
    "slice_video(file_name, [(0, 240, 'hanni.mp4')], height=540, width=960)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV 설치\n",
    "- `pip uninstall opencv-python`\n",
    "- `pip install opencv-contrib-python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection & Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image = \"../face_recognition/test_hanni2.jpg\"\n",
    "target_video = \"hanni.mp4\"\n",
    "\n",
    "import cv2\n",
    "import face_recognition\n",
    "\n",
    "# Load the known image and encode it\n",
    "known_image = face_recognition.load_image_file(base_image)\n",
    "known_face_encoding = face_recognition.face_encodings(known_image)[0]\n",
    "\n",
    "# Load the video file\n",
    "video_capture = cv2.VideoCapture(target_video)\n",
    "\n",
    "# Initialize variables\n",
    "trackers = []\n",
    "face_names = []\n",
    "face_distances = []\n",
    "frame_skip = 36\n",
    "frame_count = 0\n",
    "specific_person_present = False  # Flag to indicate if Specific Person is present\n",
    "\n",
    "def expand_bounding_box(left, top, width, height, frame_width, frame_height, expand_ratio=0.25):\n",
    "    expand_width = int(width * expand_ratio)\n",
    "    expand_height = int(height * expand_ratio)\n",
    "    \n",
    "    expanded_left = int(max(0, left - expand_width))\n",
    "    expanded_top = int(max(0, top - expand_height))\n",
    "    expanded_right = int(min(frame_width, left + width + expand_width))\n",
    "    expanded_bottom = int(min(frame_height, top + height + expand_height))\n",
    "    \n",
    "    return expanded_left, expanded_top, expanded_right, expanded_bottom\n",
    "\n",
    "def draw_face_annotations(frame, name, distance, bbox, frame_width, frame_height):\n",
    "    left, top, width, height = bbox\n",
    "    expanded_left, expanded_top, expanded_right, expanded_bottom = expand_bounding_box(\n",
    "        left, top, width, height, frame_width, frame_height)\n",
    "    \n",
    "    # Convert coordinates to integers\n",
    "    expanded_left = int(expanded_left)\n",
    "    expanded_top = int(expanded_top)\n",
    "    expanded_right = int(expanded_right)\n",
    "    expanded_bottom = int(expanded_bottom)\n",
    "    \n",
    "    # Set color based on name\n",
    "    if name == \"Specific Person\":\n",
    "        color = (0, 0, 255)  # Red\n",
    "    elif name == \"Candidate\":\n",
    "        color = (255, 0, 0)  # Blue\n",
    "    else:\n",
    "        color = (0, 255, 0)  # Green\n",
    "    \n",
    "    # Draw rectangle\n",
    "    cv2.rectangle(frame, (expanded_left, expanded_top), (expanded_right, expanded_bottom), color, 2)\n",
    "    \n",
    "    # Display name and distance\n",
    "    cv2.putText(frame, name, (expanded_left, expanded_bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "    cv2.putText(frame, f\"Distance: {distance:.2f}\", (expanded_left, expanded_bottom + 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "while True:\n",
    "    # Read a single frame from the video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Get frame dimensions\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "    if not specific_person_present:\n",
    "        # Attempt face detection in every frame until Specific Person is found\n",
    "        trackers = []\n",
    "        face_names = []\n",
    "        face_distances = []\n",
    "\n",
    "        # Detect face locations and encodings\n",
    "        face_locations = face_recognition.face_locations(rgb_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "        distances = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # Calculate face distance\n",
    "            distance = face_recognition.face_distance([known_face_encoding], face_encoding)[0]\n",
    "            distances.append(distance)\n",
    "\n",
    "        # Set tolerance (e.g., 0.46)\n",
    "        tolerance = 0.46\n",
    "\n",
    "        # Find the index of the Specific Person\n",
    "        specific_person_index = None\n",
    "        if len(distances) > 0:\n",
    "            min_distance = min(distances)\n",
    "            if min_distance < tolerance:\n",
    "                specific_person_index = distances.index(min_distance)\n",
    "\n",
    "        if specific_person_index is not None:\n",
    "            # Specific Person detected\n",
    "            specific_person_present = True\n",
    "\n",
    "            for idx, (face_location, face_encoding) in enumerate(zip(face_locations, face_encodings)):\n",
    "                top, right, bottom, left = face_location\n",
    "\n",
    "                # Calculate face distance\n",
    "                distance = distances[idx]\n",
    "\n",
    "                if distance < tolerance:\n",
    "                    if idx == specific_person_index:\n",
    "                        name = \"Specific Person\"\n",
    "                    else:\n",
    "                        name = \"Candidate\"\n",
    "                else:\n",
    "                    name = \"Unknown\"\n",
    "\n",
    "                # Initialize tracker\n",
    "                tracker = cv2.legacy.TrackerKCF_create()\n",
    "                bbox = (left, top, right - left, bottom - top)\n",
    "                tracker.init(frame, bbox)\n",
    "                trackers.append(tracker)\n",
    "                face_names.append(name)\n",
    "                face_distances.append(distance)\n",
    "\n",
    "                # Draw face annotations\n",
    "                draw_face_annotations(frame, name, distance, bbox, frame_width, frame_height)\n",
    "        else:\n",
    "            # Specific Person not detected; do not proceed with tracking\n",
    "            pass\n",
    "    else:\n",
    "        # Specific Person is being tracked\n",
    "        if frame_count % frame_skip == 0:\n",
    "            # Reset trackers and face information\n",
    "            trackers = []\n",
    "            face_names = []\n",
    "            face_distances = []\n",
    "\n",
    "            # Detect face locations and encodings\n",
    "            face_locations = face_recognition.face_locations(rgb_frame)\n",
    "            face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "            distances = []\n",
    "            for face_encoding in face_encodings:\n",
    "                # Calculate face distance\n",
    "                distance = face_recognition.face_distance([known_face_encoding], face_encoding)[0]\n",
    "                distances.append(distance)\n",
    "\n",
    "            # Set tolerance (e.g., 0.46)\n",
    "            tolerance = 0.46\n",
    "\n",
    "            # Find the index of the Specific Person\n",
    "            specific_person_index = None\n",
    "            if len(distances) > 0:\n",
    "                min_distance = min(distances)\n",
    "                if min_distance < tolerance:\n",
    "                    specific_person_index = distances.index(min_distance)\n",
    "                else:\n",
    "                    specific_person_index = None\n",
    "\n",
    "            if specific_person_index is not None:\n",
    "                # Specific Person still detected\n",
    "                for idx, (face_location, face_encoding) in enumerate(zip(face_locations, face_encodings)):\n",
    "                    top, right, bottom, left = face_location\n",
    "\n",
    "                    # Calculate face distance\n",
    "                    distance = distances[idx]\n",
    "\n",
    "                    if distance < tolerance:\n",
    "                        if idx == specific_person_index:\n",
    "                            name = \"Specific Person\"\n",
    "                        else:\n",
    "                            name = \"Candidate\"\n",
    "                    else:\n",
    "                        name = \"Unknown\"\n",
    "\n",
    "                    # Initialize tracker\n",
    "                    tracker = cv2.legacy.TrackerKCF_create()\n",
    "                    bbox = (left, top, right - left, bottom - top)\n",
    "                    tracker.init(frame, bbox)\n",
    "                    trackers.append(tracker)\n",
    "                    face_names.append(name)\n",
    "                    face_distances.append(distance)\n",
    "\n",
    "                    # Draw face annotations\n",
    "                    draw_face_annotations(frame, name, distance, bbox, frame_width, frame_height)\n",
    "            else:\n",
    "                # Specific Person lost\n",
    "                specific_person_present = False\n",
    "                trackers = []\n",
    "                face_names = []\n",
    "                face_distances = []\n",
    "        else:\n",
    "            # Update all trackers\n",
    "            new_trackers = []\n",
    "            new_face_names = []\n",
    "            new_face_distances = []\n",
    "\n",
    "            specific_person_still_present = False\n",
    "\n",
    "            for tracker, name, distance in zip(trackers, face_names, face_distances):\n",
    "                success, bbox = tracker.update(frame)\n",
    "                if success:\n",
    "                    new_trackers.append(tracker)\n",
    "                    new_face_names.append(name)\n",
    "                    new_face_distances.append(distance)\n",
    "\n",
    "                    # Convert bbox to integers\n",
    "                    bbox = tuple(map(int, bbox))\n",
    "\n",
    "                    # Draw face annotations\n",
    "                    draw_face_annotations(frame, name, distance, bbox, frame_width, frame_height)\n",
    "\n",
    "                    if name == \"Specific Person\":\n",
    "                        specific_person_still_present = True\n",
    "                else:\n",
    "                    if name == \"Specific Person\":\n",
    "                        specific_person_still_present = False\n",
    "\n",
    "            # Update trackers and face info\n",
    "            trackers = new_trackers\n",
    "            face_names = new_face_names\n",
    "            face_distances = new_face_distances\n",
    "\n",
    "            if not specific_person_still_present:\n",
    "                # Specific Person lost during tracking\n",
    "                specific_person_present = False\n",
    "                trackers = []\n",
    "                face_names = []\n",
    "                face_distances = []\n",
    "\n",
    "    # Display the video\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Exit if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup after finishing\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Diffusion Web UI API 를 사용하여 Face Swapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image = \"../face_recognition/test_hanni2.jpg\"\n",
    "source_image = \"../face_recognition/faces/김태희.jpg\"\n",
    "target_video = \"hanni.mp4\"\n",
    "\n",
    "import io\n",
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "from get_swapped_face import get_swapped_face\n",
    "\n",
    "# 특정 인물의 이미지를 로드하여 인코딩\n",
    "known_image = face_recognition.load_image_file(base_image)\n",
    "known_face_encoding = face_recognition.face_encodings(known_image)[0]\n",
    "\n",
    "with open(source_image, \"rb\") as source_file:\n",
    "    source_bytes = source_file.read()\n",
    "\n",
    "# 비디오 파일 로드\n",
    "video_capture = cv2.VideoCapture(target_video)\n",
    "\n",
    "# 변수 초기화\n",
    "trackers = []\n",
    "face_names = []\n",
    "face_distances = []\n",
    "frame_skip = 12\n",
    "frame_count = 0\n",
    "\n",
    "# swap_face 함수가 이미 존재한다고 가정합니다.\n",
    "# def swap_face(jpg_binary):\n",
    "#     # 얼굴 스왑 처리 후 동일한 크기의 jpg_binary 반환\n",
    "#     return swapped_jpg_binary\n",
    "\n",
    "while True:\n",
    "    # 비디오에서 한 프레임씩 읽기\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # 프레임의 높이와 너비를 가져오기\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "    if frame_count % frame_skip == 0:\n",
    "        # 추적기와 얼굴 정보 초기화\n",
    "        trackers = []\n",
    "        face_names = []\n",
    "        face_distances = []\n",
    "\n",
    "        # 얼굴 위치와 인코딩 탐지\n",
    "        face_locations = face_recognition.face_locations(rgb_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "        distances = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # 얼굴 거리 계산\n",
    "            distance = face_recognition.face_distance([known_face_encoding], face_encoding)[0]\n",
    "            distances.append(distance)\n",
    "\n",
    "        # 임계값 설정 (예: 0.46)\n",
    "        tolerance = 0.46\n",
    "\n",
    "        # 거리 기준으로 가장 작은 인덱스 찾기\n",
    "        specific_person_index = None\n",
    "        if len(distances) > 0:\n",
    "            min_distance = min(distances)\n",
    "            if min_distance < tolerance:\n",
    "                specific_person_index = distances.index(min_distance)\n",
    "\n",
    "        for idx, (face_location, face_encoding) in enumerate(zip(face_locations, face_encodings)):\n",
    "            top, right, bottom, left = face_location\n",
    "\n",
    "            # 얼굴 거리 계산\n",
    "            distance = distances[idx]\n",
    "\n",
    "            if distance < tolerance:\n",
    "                if idx == specific_person_index:\n",
    "                    name = \"Specific Person\"\n",
    "                else:\n",
    "                    name = \"Candidate\"\n",
    "            else:\n",
    "                name = \"Unknown\"\n",
    "\n",
    "            # 추적기 초기화\n",
    "            tracker = cv2.legacy.TrackerKCF_create()\n",
    "            bbox = (left, top, right - left, bottom - top)\n",
    "            tracker.init(frame, bbox)\n",
    "            trackers.append(tracker)\n",
    "            face_names.append(name)\n",
    "            face_distances.append(distance)\n",
    "    else:\n",
    "        # 모든 추적기 업데이트\n",
    "        new_trackers = []\n",
    "        new_face_names = []\n",
    "        new_face_distances = []\n",
    "\n",
    "        for tracker, name, distance in zip(trackers, face_names, face_distances):\n",
    "            success, bbox = tracker.update(frame)\n",
    "            if success:\n",
    "                new_trackers.append(tracker)\n",
    "                new_face_names.append(name)\n",
    "                new_face_distances.append(distance)\n",
    "                left, top, width, height = [int(v) for v in bbox]\n",
    "                right = left + width\n",
    "                bottom = top + height\n",
    "\n",
    "                # 사각형을 4방으로 1.5배 확장\n",
    "                expand_width = int(width * 0.25)  # 원래의 0.25배씩 좌우로 확장\n",
    "                expand_height = int(height * 0.25)  # 원래의 0.25배씩 상하로 확장\n",
    "\n",
    "                # 새로운 좌표 계산 (프레임 경계를 넘지 않도록 조정)\n",
    "                expanded_left = max(0, left - expand_width)\n",
    "                expanded_top = max(0, top - expand_height)\n",
    "                expanded_right = min(frame_width, right + expand_width)\n",
    "                expanded_bottom = min(frame_height, bottom + expand_height)\n",
    "\n",
    "                # 사각형과 텍스트 표시\n",
    "                if name == \"Specific Person\":\n",
    "                    # 특정 인물에 대해 얼굴 스왑 처리\n",
    "\n",
    "                    # 확장된 사각형 영역 추출\n",
    "                    face_region = frame[expanded_top:expanded_bottom, expanded_left:expanded_right]\n",
    "\n",
    "                    # 이미지를 JPG 바이너리로 인코딩\n",
    "                    success_enc, jpg_buffer = cv2.imencode('.jpg', face_region)\n",
    "                    if success_enc:\n",
    "                        jpg_binary = jpg_buffer.tobytes()\n",
    "\n",
    "                        # swap_face 함수 호출\n",
    "                        swap_png_binary = get_swapped_face(source_bytes, jpg_binary)\n",
    "\n",
    "                        # swap_png_binary가 BytesIO 객체인 경우 처리\n",
    "                        if isinstance(swap_png_binary, io.BytesIO):\n",
    "                            swap_png_binary = swap_png_binary.getvalue()\n",
    "\n",
    "                        # swap_png_binary를 이미지로 디코딩\n",
    "                        swap_image_array = np.frombuffer(swap_png_binary, np.uint8)\n",
    "                        swap_image = cv2.imdecode(swap_image_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "                        # 스왑된 이미지의 크기가 영역과 다를 수 있으므로 리사이즈\n",
    "                        swap_image = cv2.resize(swap_image, (expanded_right - expanded_left, expanded_bottom - expanded_top))\n",
    "\n",
    "                        # 스왑된 이미지를 프레임에 적용\n",
    "                        frame[expanded_top:expanded_bottom, expanded_left:expanded_right] = swap_image\n",
    "\n",
    "                    # 사각형 그리기\n",
    "                    color = (0, 0, 255)  # 빨간색\n",
    "                    cv2.rectangle(frame, (expanded_left, expanded_top), (expanded_right, expanded_bottom), color, 2)\n",
    "                    # 이름 표시\n",
    "                    cv2.putText(frame, name, (expanded_left, expanded_bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                    # 거리 표시 (이름 아래)\n",
    "                    cv2.putText(frame, f\"Distance: {distance:.2f}\", (expanded_left, expanded_bottom + 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "                elif name == \"Candidate\":\n",
    "                    # 후보자는 파란색 사각형과 이름 표시\n",
    "                    color = (255, 0, 0)\n",
    "                    cv2.rectangle(frame, (expanded_left, expanded_top), (expanded_right, expanded_bottom), color, 2)\n",
    "                    cv2.putText(frame, name, (expanded_left, expanded_bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                    cv2.putText(frame, f\"Distance: {distance:.2f}\", (expanded_left, expanded_bottom + 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                else:\n",
    "                    # 다른 인물에 대해 초록색 사각형과 이름만 표시\n",
    "                    color = (0, 255, 0)\n",
    "                    cv2.rectangle(frame, (expanded_left, expanded_top), (expanded_right, expanded_bottom), color, 2)\n",
    "                    cv2.putText(frame, name, (expanded_left, expanded_bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                    cv2.putText(frame, f\"Distance: {distance:.2f}\", (expanded_left, expanded_bottom + 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        # 추적기와 얼굴 정보 업데이트\n",
    "        trackers = new_trackers\n",
    "        face_names = new_face_names\n",
    "        face_distances = new_face_distances\n",
    "\n",
    "    # 비디오 출력\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 모든 작업 완료 후 클린업\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### insightface library 를 사용하여 Face Swapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    pip install insightface\n",
    "    pip install onnxruntime #for CPU-only\n",
    "    pip install onnxruntime-gpu #For GPU\n",
    "    pip uninstall opencv-python-headless\n",
    "    pip install opencv-contrib-python-headless # 또는 opencv-contrib-python\n",
    "```\n",
    "- Download `inswapper_128.onnx` & Locate it in a specific directory\n",
    "- buffalo_l download from : \n",
    "    https://github.com/deepinsight/insightface/releases\n",
    "\n",
    "- unzip buffalo_l.zip on `C:\\Users\\<user>\\.insightface\\models\\buffalo_l`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "from inswapper import FaceSwapper\n",
    "\n",
    "base_image = \"../face_recognition/test_hanni2.jpg\"\n",
    "source_image = \"../face_recognition/faces/yunsy.jpg\"\n",
    "target_video = \"hanni.mp4\"\n",
    "\n",
    "# 특정 인물의 이미지를 로드하여 인코딩\n",
    "known_image = face_recognition.load_image_file(base_image)\n",
    "known_face_encoding = face_recognition.face_encodings(known_image)[0]\n",
    "\n",
    "# FaceSwapper 클래스 인스턴스 생성\n",
    "face_swapper = FaceSwapper(det_size=(160, 160))\n",
    "\n",
    "# 소스 얼굴 설정 (face_index는 선택 사항)\n",
    "success = face_swapper.set_source_face(source_image)\n",
    "\n",
    "if not success:\n",
    "    print(\"소스 얼굴 설정에 실패했습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 비디오 파일 로드\n",
    "video_capture = cv2.VideoCapture(target_video)\n",
    "\n",
    "# 변수 초기화\n",
    "trackers = []\n",
    "face_names = []\n",
    "face_distances = []\n",
    "frame_skip = 24\n",
    "frame_count = 0\n",
    "\n",
    "def process_face(frame, name, distance, bbox, frame_width, frame_height):\n",
    "    left, top, width, height = bbox\n",
    "    expand_ratio=0.3\n",
    "    expand_width = int(width * expand_ratio)\n",
    "    expand_height = int(height * expand_ratio)\n",
    "\n",
    "    expanded_left = int(max(0, left - expand_width))\n",
    "    expanded_top = int(max(0, top - expand_height))\n",
    "    expanded_right = int(min(frame_width, left + width + expand_width))\n",
    "    expanded_bottom = int(min(frame_height, top + height + expand_height))\n",
    "\n",
    "    # 얼굴 영역 추출\n",
    "    face_region = frame[expanded_top:expanded_bottom, expanded_left:expanded_right]\n",
    "\n",
    "    if name == \"Specific Person\":\n",
    "        # 얼굴 스왑 처리\n",
    "        swap_image = face_swapper.swap_faces_in_image(face_region)\n",
    "        if swap_image is not None:\n",
    "            # 스왑된 이미지를 얼굴 영역 크기에 맞게 조정\n",
    "            swap_image = cv2.resize(swap_image, (expanded_right - expanded_left, expanded_bottom - expanded_top))\n",
    "            # 스왑된 얼굴을 프레임에 적용\n",
    "            frame[expanded_top:expanded_bottom, expanded_left:expanded_right] = swap_image\n",
    "        color = (0, 0, 255)  # 빨간색\n",
    "    elif name == \"Candidate\":\n",
    "        color = (255, 0, 0)  # 파란색\n",
    "    else:\n",
    "        color = (0, 255, 0)  # 초록색\n",
    "\n",
    "    # 사각형 그리기\n",
    "    cv2.rectangle(frame, (expanded_left, expanded_top), (expanded_right, expanded_bottom), color, 2)\n",
    "    # 이름과 거리 표시\n",
    "    cv2.putText(frame, name, (expanded_left, expanded_bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "    cv2.putText(frame, f\"Distance: {distance:.2f}\", (expanded_left, expanded_bottom + 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "while True:\n",
    "    # 비디오에서 한 프레임씩 읽기\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # 프레임의 높이와 너비를 가져오기\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "    if frame_count % frame_skip == 0:\n",
    "        # 추적기와 얼굴 정보 초기화\n",
    "        trackers = []\n",
    "        face_names = []\n",
    "        face_distances = []\n",
    "\n",
    "        # 얼굴 위치와 인코딩 탐지\n",
    "        face_locations = face_recognition.face_locations(rgb_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "        distances = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # 얼굴 거리 계산\n",
    "            distance = face_recognition.face_distance([known_face_encoding], face_encoding)[0]\n",
    "            distances.append(distance)\n",
    "\n",
    "        # 임계값 설정 (예: 0.46)\n",
    "        tolerance = 0.46\n",
    "\n",
    "        # 거리 기준으로 가장 작은 인덱스 찾기\n",
    "        specific_person_index = None\n",
    "        if len(distances) > 0:\n",
    "            min_distance = min(distances)\n",
    "            if min_distance < tolerance:\n",
    "                specific_person_index = distances.index(min_distance)\n",
    "\n",
    "        for idx, (face_location, face_encoding) in enumerate(zip(face_locations, face_encodings)):\n",
    "            top, right, bottom, left = face_location\n",
    "            width = right - left\n",
    "            height = bottom - top\n",
    "            bbox = (left, top, width, height)\n",
    "\n",
    "            # 얼굴 거리 계산\n",
    "            distance = distances[idx]\n",
    "\n",
    "            if distance < tolerance:\n",
    "                if idx == specific_person_index:\n",
    "                    name = \"Specific Person\"\n",
    "                else:\n",
    "                    name = \"Candidate\"\n",
    "            else:\n",
    "                name = \"Unknown\"\n",
    "\n",
    "            # 추적기 초기화\n",
    "            tracker = cv2.legacy.TrackerKCF_create()\n",
    "            tracker.init(frame, bbox)\n",
    "            trackers.append(tracker)\n",
    "            face_names.append(name)\n",
    "            face_distances.append(distance)\n",
    "\n",
    "            # 얼굴 처리 및 주석 그리기\n",
    "            process_face(frame, name, distance, bbox, frame_width, frame_height)\n",
    "\n",
    "    else:\n",
    "        # 모든 추적기 업데이트\n",
    "        new_trackers = []\n",
    "        new_face_names = []\n",
    "        new_face_distances = []\n",
    "\n",
    "        for tracker, name, distance in zip(trackers, face_names, face_distances):\n",
    "            success, bbox = tracker.update(frame)\n",
    "            if success:\n",
    "                bbox = tuple(map(int, bbox))\n",
    "                new_trackers.append(tracker)\n",
    "                new_face_names.append(name)\n",
    "                new_face_distances.append(distance)\n",
    "\n",
    "                # 얼굴 처리 및 주석 그리기\n",
    "                process_face(frame, name, distance, bbox, frame_width, frame_height)\n",
    "\n",
    "        # 추적기와 얼굴 정보 업데이트\n",
    "        trackers = new_trackers\n",
    "        face_names = new_face_names\n",
    "        face_distances = new_face_distances\n",
    "\n",
    "    # 비디오 출력\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 모든 작업 완료 후 클린업\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class 화 & Decorator 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "from inswapper import FaceSwapper\n",
    "\n",
    "class VideoFaceSwapper:\n",
    "    def __init__(self, base_image, target_video):\n",
    "\n",
    "        # Load and encode the known image\n",
    "        known_image = face_recognition.load_image_file(base_image)\n",
    "        self.known_face_encoding = face_recognition.face_encodings(known_image)[0]\n",
    "        self.target_video = target_video\n",
    "\n",
    "    def video_swap(self, func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Video capture\n",
    "            video_capture = cv2.VideoCapture(self.target_video)\n",
    "\n",
    "            # Initialize variables\n",
    "            trackers = []\n",
    "            face_names = []\n",
    "            face_distances = []\n",
    "            frame_skip = 24\n",
    "            frame_count = 0\n",
    "\n",
    "            while True:\n",
    "                ret, frame = video_capture.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                frame_count += 1\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                if frame_count % frame_skip == 0:\n",
    "                    trackers = []\n",
    "                    face_names = []\n",
    "                    face_distances = []\n",
    "\n",
    "                    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "                    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "                    distances = []\n",
    "                    for face_encoding in face_encodings:\n",
    "                        distance = face_recognition.face_distance([self.known_face_encoding], face_encoding)[0]\n",
    "                        distances.append(distance)\n",
    "\n",
    "                    tolerance = 0.46\n",
    "                    specific_person_index = None\n",
    "                    if len(distances) > 0:\n",
    "                        min_distance = min(distances)\n",
    "                        if min_distance < tolerance:\n",
    "                            specific_person_index = distances.index(min_distance)\n",
    "\n",
    "                    for idx, (face_location, face_encoding) in enumerate(zip(face_locations, face_encodings)):\n",
    "                        top, right, bottom, left = face_location\n",
    "                        width = right - left\n",
    "                        height = bottom - top\n",
    "                        bbox = (left, top, width, height)\n",
    "\n",
    "                        distance = distances[idx]\n",
    "                        if distance < tolerance:\n",
    "                            if idx == specific_person_index:\n",
    "                                name = \"Specific Person\"\n",
    "                            else:\n",
    "                                name = \"Candidate\"\n",
    "                        else:\n",
    "                            name = \"Unknown\"\n",
    "\n",
    "                        # Initialize tracker\n",
    "                        tracker = cv2.legacy.TrackerKCF_create()\n",
    "                        tracker.init(frame, bbox)\n",
    "                        trackers.append(tracker)\n",
    "                        face_names.append(name)\n",
    "                        face_distances.append(distance)\n",
    "\n",
    "                        # Process face and annotate\n",
    "                        self.annotate_frame(bbox, frame, name, distance, func)\n",
    "                else:\n",
    "                    new_trackers = []\n",
    "                    new_face_names = []\n",
    "                    new_face_distances = []\n",
    "\n",
    "                    for tracker, name, distance in zip(trackers, face_names, face_distances):\n",
    "                        success, bbox = tracker.update(frame)\n",
    "                        if success:\n",
    "                            bbox = tuple(map(int, bbox))\n",
    "                            new_trackers.append(tracker)\n",
    "                            new_face_names.append(name)\n",
    "                            new_face_distances.append(distance)\n",
    "\n",
    "                            # Process face and annotate\n",
    "                            self.annotate_frame(bbox, frame, name, distance, func)\n",
    "                        else:\n",
    "                            print(\"Tracker lost!\")\n",
    "\n",
    "                    trackers = new_trackers\n",
    "                    face_names = new_face_names\n",
    "                    face_distances = new_face_distances\n",
    "\n",
    "                # Display the video\n",
    "                cv2.imshow('Video', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            # Cleanup\n",
    "            video_capture.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    def annotate_frame(self, bbox, frame, name, distance, func):\n",
    "        left, top, width, height = bbox\n",
    "        expand_ratio = 0.3\n",
    "        expand_width = int(width * expand_ratio)\n",
    "        expand_height = int(height * expand_ratio)\n",
    "\n",
    "        expanded_left = int(max(0, left - expand_width))\n",
    "        expanded_top = int(max(0, top - expand_height))\n",
    "\n",
    "        frame_height, frame_width, _ = frame.shape\n",
    "        expanded_right = int(min(frame_width, left + width + expand_width))\n",
    "        expanded_bottom = int(min(frame_height, top + height + expand_height))\n",
    "\n",
    "        # Extract face region\n",
    "        face_region = frame[expanded_top:expanded_bottom, expanded_left:expanded_right]\n",
    "\n",
    "        if name == \"Specific Person\":\n",
    "            # Apply face swap\n",
    "            swap_image = func(face_region)\n",
    "            # Replace the face region in the frame\n",
    "            frame[expanded_top:expanded_bottom, expanded_left:expanded_right] = swap_image\n",
    "            color = (0, 0, 255)  # Red\n",
    "        elif name == \"Candidate\":\n",
    "            color = (255, 0, 0)  # Blue\n",
    "        else:\n",
    "            color = (0, 255, 0)  # Green\n",
    "\n",
    "        # Draw rectangle and annotations\n",
    "        cv2.rectangle(frame, (expanded_left, expanded_top), (expanded_right, expanded_bottom), color, 2)\n",
    "        cv2.putText(frame, name, (expanded_left, expanded_bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        cv2.putText(frame, f\"Distance: {distance:.2f}\", (expanded_left, expanded_bottom + 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "# Initialize the face swapper and known face encoding outside the class\n",
    "base_image = \"../face_recognition/test_hanni2.jpg\"\n",
    "target_video = \"hanni.mp4\"\n",
    "\n",
    "# Create an instance of VideoFaceSwapper\n",
    "swapper = VideoFaceSwapper(base_image, target_video)\n",
    "\n",
    "# Create FaceSwapper instance\n",
    "face_swapper = FaceSwapper(det_size=(160, 160))\n",
    "\n",
    "# Set the source face\n",
    "source_image = \"../face_recognition/faces/yunsy.jpg\"\n",
    "success = face_swapper.set_source_face(source_image)\n",
    "if not success:\n",
    "    print(\"Failed to set source face.\")\n",
    "    exit()\n",
    "\n",
    "@swapper.video_swap\n",
    "def swap_other_face(face_region):\n",
    "    swap_image = face_swapper.swap_faces_in_image(face_region)\n",
    "    return swap_image if swap_image is not None else face_region\n",
    "\n",
    "# Start the face-swapping process\n",
    "swap_other_face()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### insightface 를 사용한 Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 얼굴 인식을 위해 InsightFace를 사용하는 샘플 코드\n",
    "\n",
    "import cv2\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# FaceAnalysis 객체 초기화 (사전 학습된 모델 사용)\n",
    "app = FaceAnalysis(name='buffalo_l')  # 'buffalo_l'는 사전 학습된 모델 이름입니다.\n",
    "app.prepare(ctx_id=-1)  # ctx_id=0은 GPU 사용, ctx_id=-1은 CPU 사용\n",
    "app.det_model.nms_thresh = 0.4  # NMS 임계값 설정\n",
    "\n",
    "# 이미지 파일 읽기\n",
    "img = cv2.imread(\"../face_recognition/faces/bk.goldengirls01.jpg\")  # 처리할 이미지 파일의 경로로 변경하세요.\n",
    "if img is None:\n",
    "    raise FileNotFoundError(f\"이미지를 불러올 수 없습니다. 경로를 확인하세요\")\n",
    "\n",
    "# 얼굴 검출 및 임베딩 추출\n",
    "faces = app.get(img)\n",
    "\n",
    "# 검출된 얼굴 처리\n",
    "for idx, face in enumerate(faces):\n",
    "    # 얼굴 영역 표시\n",
    "    bbox = face.bbox.astype(int)\n",
    "    cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "    # 얼굴 임베딩 출력\n",
    "    #print(f\"얼굴 {idx+1} 임베딩 벡터:\\n{face.embedding}\")\n",
    "\n",
    "# 결과 이미지 표시\n",
    "cv2.imshow('얼굴 인식 결과', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# FaceAnalysis 객체 초기화 (사전 학습된 모델 사용)\n",
    "app = FaceAnalysis(name='buffalo_l')  # 'buffalo_l'는 사전 학습된 모델 이름입니다.\n",
    "app.prepare(ctx_id=0)  # ctx_id=0은 GPU 사용, ctx_id=-1은 CPU 사용\n",
    "\n",
    "# 비교할 기준 얼굴 이미지 로드 및 임베딩 추출\n",
    "ref_img_path = \"../face_recognition/faces/hanni01.jpg\"  # 기준 얼굴 이미지 경로로 변경하세요.\n",
    "ref_img = cv2.imread(ref_img_path)\n",
    "if ref_img is None:\n",
    "    raise FileNotFoundError(f\"기준 이미지를 불러올 수 없습니다. 경로를 확인하세요: {ref_img_path}\")\n",
    "\n",
    "ref_faces = app.get(ref_img)\n",
    "if len(ref_faces) == 0:\n",
    "    raise ValueError(\"기준 이미지에서 얼굴을 검출하지 못했습니다.\")\n",
    "\n",
    "# 기준 얼굴의 임베딩 추출 (첫 번째 얼굴 사용)\n",
    "ref_embedding = ref_faces[0].embedding\n",
    "\n",
    "# 비교할 대상 이미지 로드 및 얼굴 임베딩 추출\n",
    "target_img_path = \"../face_recognition/faces/newjeans02_g.jpg\"  # 대상 이미지 경로로 변경하세요.\n",
    "target_img = cv2.imread(target_img_path)\n",
    "if target_img is None:\n",
    "    raise FileNotFoundError(f\"대상 이미지를 불러올 수 없습니다. 경로를 확인하세요: {target_img_path}\")\n",
    "\n",
    "target_faces = app.get(target_img)\n",
    "\n",
    "# target_faces를 x축 기준으로 정렬 (좌에서 우로)\n",
    "target_faces.sort(key=lambda face: face.bbox[0])\n",
    "\n",
    "# 검출된 얼굴들에 대해 유사도 계산 및 표시\n",
    "for idx, face in enumerate(target_faces):\n",
    "    # 대상 얼굴의 임베딩 추출\n",
    "    target_embedding = face.embedding\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    similarity = cosine_similarity([ref_embedding], [target_embedding])[0][0]\n",
    "\n",
    "    # 유사도 출력\n",
    "    print(f\"얼굴 {idx+1} 유사도: {similarity:.4f}\")\n",
    "\n",
    "    # 얼굴 영역 표시\n",
    "    bbox = face.bbox.astype(int)\n",
    "    cv2.rectangle(target_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "    # 유사도 텍스트 표시\n",
    "    cv2.putText(target_img, f\"{idx} : {similarity:.2f}\", (bbox[0], bbox[1]-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# 결과 이미지 표시\n",
    "cv2.imshow('Similarity Result', target_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mp4 파일 만들기 옵션 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.15 (you have 1.4.11). Upgrade using: pip install --upgrade albumentations\n",
      "c:\\Python310\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:65: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tanmi/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tanmi/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tanmi/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tanmi/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tanmi/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tanmi/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tanmi/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tanmi/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tanmi/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\tanmi/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (320, 320)\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "inswapper-shape: [1, 3, 128, 128]\n",
      "소스 얼굴이 설정되었습니다. 인덱스: 0\n",
      "specific_person_present :False\n",
      "face count : 3 similarities : [0.52120537, 0.20202701, 0.080400154] specific_person_index : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :True\n",
      "specific_person_present :False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face count : 3 similarities : [0.1889267, 0.36581963, 0.20342335] specific_person_index : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specific_person_present :True\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from inswapper import FaceSwapper\n",
    "\n",
    "class VideoFaceSwapper:\n",
    "    def __init__(self, base_image, target_video, tolerance=0.35, output_video=None, display_video=True, display_rectangle=True, segments=None):\n",
    "        # Initialize FaceAnalysis object\n",
    "        self.app = FaceAnalysis(name='buffalo_l')\n",
    "        self.app.prepare(ctx_id=0)  # 0 Use GPU (set ctx_id=-1 to use CPU)\n",
    "\n",
    "        # Load the reference face image and extract embedding\n",
    "        ref_img = cv2.imread(base_image)\n",
    "        if ref_img is None:\n",
    "            raise FileNotFoundError(f\"Unable to load the reference image: {base_image}\")\n",
    "\n",
    "        ref_faces = self.app.get(ref_img)\n",
    "        if len(ref_faces) == 0:\n",
    "            raise ValueError(\"No faces detected in the reference image.\")\n",
    "\n",
    "        # Extract embedding of the reference face (use the first face)\n",
    "        self.known_face_embedding = ref_faces[0].embedding\n",
    "        self.target_video = target_video\n",
    "        self.output_video = output_video\n",
    "        self.display_video = display_video\n",
    "        self.display_rectangle = display_rectangle\n",
    "        self.tolerance = tolerance\n",
    "        self.specific_person_present = False  # Flag to indicate if Specific Person is present\n",
    "\n",
    "        # Segments to process\n",
    "        self.segments = segments  # List of tuples (start_time_str, duration_in_seconds)\n",
    "\n",
    "        # If output_video is None, do not use video saving feature\n",
    "        if self.output_video is not None:\n",
    "            # Check and create the directory for output_video\n",
    "            output_dir = os.path.dirname(self.output_video)\n",
    "            if output_dir and not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            # Check file extension\n",
    "            _, ext = os.path.splitext(self.output_video)\n",
    "            ext = ext.lower()\n",
    "            if ext == '.webm':\n",
    "                self.fourcc = cv2.VideoWriter_fourcc(*'VP90')  # WebM format codec\n",
    "            else:\n",
    "                self.fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Default codec\n",
    "\n",
    "    def video_swap(self, func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Video capture\n",
    "            video_capture = cv2.VideoCapture(self.target_video)\n",
    "\n",
    "            # Get video properties\n",
    "            fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "            width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))  # Total number of frames\n",
    "\n",
    "            # Set up video writer if output_video is specified\n",
    "            if self.output_video:\n",
    "                video_writer = cv2.VideoWriter(self.output_video, self.fourcc, fps, (width, height))\n",
    "\n",
    "            # Initialize variables\n",
    "            trackers = []\n",
    "            face_names = []\n",
    "            face_similarities = []\n",
    "            frame_skip = 24\n",
    "            frame_count = 0\n",
    "\n",
    "            # Convert segments to list of (start_frame, end_frame)\n",
    "            if self.segments is not None:\n",
    "                segment_frames = []\n",
    "                for start_time_str, duration in self.segments:\n",
    "                    start_seconds = self._time_str_to_seconds(start_time_str)\n",
    "                    end_seconds = start_seconds + duration\n",
    "                    start_frame = int(start_seconds * fps)\n",
    "                    end_frame = int(end_seconds * fps)\n",
    "                    segment_frames.append((start_frame, end_frame))\n",
    "            else:\n",
    "                # If no segments are provided, process the entire video\n",
    "                segment_frames = [(0, total_frames)]\n",
    "\n",
    "            # Process each segment\n",
    "            for start_frame, end_frame in segment_frames:\n",
    "                if start_frame >= total_frames:\n",
    "                    print(f\"Start frame {start_frame} exceeds total frames {total_frames}. Skipping segment.\")\n",
    "                    continue\n",
    "\n",
    "                # Adjust end_frame if it exceeds total_frames\n",
    "                if end_frame > total_frames:\n",
    "                    end_frame = total_frames\n",
    "\n",
    "                # Set video capture to the start frame\n",
    "                video_capture.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "                frame_count = start_frame\n",
    "\n",
    "                while frame_count < end_frame:\n",
    "                    ret, frame = video_capture.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "\n",
    "                    frame_count += 1\n",
    "\n",
    "                    print(f\"specific_person_present :{self.specific_person_present}\")\n",
    "\n",
    "                    if not self.specific_person_present:\n",
    "                        # Attempt to detect Specific Person in every frame\n",
    "                        trackers = []\n",
    "                        face_names = []\n",
    "                        face_similarities = []\n",
    "\n",
    "                        # Detect faces and extract embeddings\n",
    "                        faces = self.app.get(frame)\n",
    "                        \n",
    "                        if len(faces) == 0:\n",
    "                            continue  # No faces detected, skip to next frame\n",
    "\n",
    "                        similarities = []\n",
    "                        for face in faces:\n",
    "                            face_embedding = face.embedding\n",
    "\n",
    "                            # Calculate cosine similarity\n",
    "                            similarity = cosine_similarity(\n",
    "                                [self.known_face_embedding], [face_embedding]\n",
    "                            )[0][0]\n",
    "                            similarities.append(similarity)\n",
    "\n",
    "                        # Set similarity threshold\n",
    "                        tolerance = self.tolerance\n",
    "\n",
    "                        # Find the index of the Specific Person\n",
    "                        specific_person_index = None\n",
    "                        if len(similarities) > 0:\n",
    "                            max_similarity = max(similarities)\n",
    "                            if max_similarity > tolerance:\n",
    "                                specific_person_index = similarities.index(max_similarity)\n",
    "\n",
    "                        #print(f\"face count : {len(faces)} similarities : {similarities} specific_person_index : {specific_person_index}\")\n",
    "                        \n",
    "                        if specific_person_index is not None:\n",
    "                            # Specific Person detected\n",
    "                            self.specific_person_present = True\n",
    "\n",
    "                            for idx, face in enumerate(faces):\n",
    "                                bbox = face.bbox.astype(int)\n",
    "                                similarity = similarities[idx]\n",
    "\n",
    "                                if similarity > tolerance:\n",
    "                                    if idx == specific_person_index:\n",
    "                                        name = \"Specific Person\"\n",
    "                                    else:\n",
    "                                        name = \"Candidate\"\n",
    "                                else:\n",
    "                                    name = \"Unknown\"\n",
    "\n",
    "                                # Initialize tracker\n",
    "                                tracker = cv2.legacy.TrackerKCF_create()\n",
    "                                x1, y1, x2, y2 = bbox\n",
    "                                w = x2 - x1\n",
    "                                h = y2 - y1\n",
    "                                tracker_bbox = (x1, y1, w, h)\n",
    "                                tracker.init(frame, tracker_bbox)\n",
    "                                trackers.append(tracker)\n",
    "                                face_names.append(name)\n",
    "                                face_similarities.append(similarity)\n",
    "\n",
    "                                # Annotate frame and apply face swap if needed\n",
    "                                self.annotate_frame(tracker_bbox, frame, name, similarity, func)\n",
    "                        else:\n",
    "                            # Specific Person not detected, process Unknown faces\n",
    "                            self.specific_person_present = False  # Ensure the flag is False\n",
    "\n",
    "                            for idx_face, face in enumerate(faces):\n",
    "                                bbox = face.bbox.astype(int)\n",
    "                                similarity = similarities[idx_face]\n",
    "\n",
    "                                if similarity > tolerance:\n",
    "                                    name = \"Candidate\"\n",
    "                                else:\n",
    "                                    name = \"Unknown\"\n",
    "\n",
    "                                # Since we are not tracking, we do not initialize trackers\n",
    "                                # Annotate frame without applying face swap\n",
    "                                x1, y1, x2, y2 = bbox\n",
    "                                w = x2 - x1\n",
    "                                h = y2 - y1\n",
    "                                tracker_bbox = (x1, y1, w, h)\n",
    "                                # Pass func=None to indicate no face swap should be applied\n",
    "                                self.annotate_frame(tracker_bbox, frame, name, similarity, func=None)\n",
    "                    else:\n",
    "                        # Specific Person is being tracked\n",
    "                        if frame_count % frame_skip == 0:\n",
    "                            # Re-detect faces\n",
    "                            trackers = []\n",
    "                            face_names = []\n",
    "                            face_similarities = []\n",
    "\n",
    "                            faces = self.app.get(frame)\n",
    "                            if len(faces) == 0:\n",
    "                                self.specific_person_present = False\n",
    "                                continue\n",
    "\n",
    "                            similarities = []\n",
    "                            for face in faces:\n",
    "                                face_embedding = face.embedding\n",
    "\n",
    "                                # Calculate cosine similarity\n",
    "                                similarity = cosine_similarity(\n",
    "                                    [self.known_face_embedding], [face_embedding]\n",
    "                                )[0][0]\n",
    "                                similarities.append(similarity)\n",
    "\n",
    "                            # Set similarity threshold\n",
    "                            tolerance = self.tolerance\n",
    "\n",
    "                            # Find the index of the Specific Person\n",
    "                            specific_person_index = None\n",
    "                            if len(similarities) > 0:\n",
    "                                max_similarity = max(similarities)\n",
    "                                if max_similarity > tolerance:\n",
    "                                    specific_person_index = similarities.index(max_similarity)\n",
    "                                else:\n",
    "                                    specific_person_index = None\n",
    "\n",
    "                            if specific_person_index is not None:\n",
    "                                # Specific Person still detected\n",
    "                                for idx, face in enumerate(faces):\n",
    "                                    bbox = face.bbox.astype(int)\n",
    "                                    similarity = similarities[idx]\n",
    "\n",
    "                                    if similarity > tolerance:\n",
    "                                        if idx == specific_person_index:\n",
    "                                            name = \"Specific Person\"\n",
    "                                        else:\n",
    "                                            name = \"Candidate\"\n",
    "                                    else:\n",
    "                                        name = \"Unknown\"\n",
    "\n",
    "                                    # Initialize tracker\n",
    "                                    tracker = cv2.legacy.TrackerKCF_create()\n",
    "                                    x1, y1, x2, y2 = bbox\n",
    "                                    w = x2 - x1\n",
    "                                    h = y2 - y1\n",
    "                                    tracker_bbox = (x1, y1, w, h)\n",
    "                                    tracker.init(frame, tracker_bbox)\n",
    "                                    trackers.append(tracker)\n",
    "                                    face_names.append(name)\n",
    "                                    face_similarities.append(similarity)\n",
    "\n",
    "                                    # Annotate frame and apply face swap if needed\n",
    "                                    self.annotate_frame(tracker_bbox, frame, name, similarity, func)\n",
    "                            else:\n",
    "                                # Specific Person lost\n",
    "                                self.specific_person_present = False\n",
    "                                trackers = []\n",
    "                                face_names = []\n",
    "                                face_similarities = []\n",
    "                        else:\n",
    "                            # Update trackers\n",
    "                            new_trackers = []\n",
    "                            new_face_names = []\n",
    "                            new_face_similarities = []\n",
    "                            specific_person_still_present = False\n",
    "\n",
    "                            for tracker, name, similarity in zip(trackers, face_names, face_similarities):\n",
    "                                success, tracker_bbox = tracker.update(frame)\n",
    "                                if success:\n",
    "                                    tracker_bbox = tuple(map(int, tracker_bbox))\n",
    "                                    new_trackers.append(tracker)\n",
    "                                    new_face_names.append(name)\n",
    "                                    new_face_similarities.append(similarity)\n",
    "\n",
    "                                    # Annotate frame and apply face swap if needed\n",
    "                                    self.annotate_frame(tracker_bbox, frame, name, similarity, func)\n",
    "\n",
    "                                    if name == \"Specific Person\":\n",
    "                                        specific_person_still_present = True\n",
    "                                else:\n",
    "                                    if name == \"Specific Person\":\n",
    "                                        specific_person_still_present = False\n",
    "\n",
    "                            # Update trackers and face info\n",
    "                            trackers = new_trackers\n",
    "                            face_names = new_face_names\n",
    "                            face_similarities = new_face_similarities\n",
    "\n",
    "                            if not specific_person_still_present:\n",
    "                                # Specific Person lost during tracking\n",
    "                                self.specific_person_present = False\n",
    "                                trackers = []\n",
    "                                face_names = []\n",
    "                                face_similarities = []\n",
    "\n",
    "                    # Display current frame number / total frames at the top-left corner\n",
    "                    cv2.putText(frame, f\"Frame: {frame_count}/{total_frames}\", (10, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "                    # Output or display video\n",
    "                    if self.display_video:\n",
    "                        cv2.imshow('Video', frame)\n",
    "                        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                            break\n",
    "\n",
    "                    if self.output_video:\n",
    "                        video_writer.write(frame)\n",
    "\n",
    "            # Cleanup\n",
    "            video_capture.release()\n",
    "            if self.output_video:\n",
    "                video_writer.release()\n",
    "            if self.display_video:\n",
    "                cv2.destroyAllWindows()\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    def annotate_frame(self, bbox, frame, name, similarity, func):\n",
    "        left, top, width, height = map(int, bbox)\n",
    "        expand_ratio = 0.3\n",
    "        expand_width = int(width * expand_ratio)\n",
    "        expand_height = int(height * expand_ratio)\n",
    "\n",
    "        expanded_left = int(max(0, left - expand_width))\n",
    "        expanded_top = int(max(0, top - expand_height))\n",
    "\n",
    "        frame_height, frame_width, _ = frame.shape\n",
    "        expanded_right = int(min(frame_width, left + width + expand_width))\n",
    "        expanded_bottom = int(min(frame_height, top + height + expand_height))\n",
    "\n",
    "        # Extract face region\n",
    "        face_region = frame[expanded_top:expanded_bottom, expanded_left:expanded_right]\n",
    "\n",
    "        if name == \"Specific Person\" and func is not None:\n",
    "            # Apply face swap\n",
    "            swap_image = func(face_region)\n",
    "            # Replace the face region with the swapped image\n",
    "            if swap_image is not None:\n",
    "                swap_image_resized = cv2.resize(swap_image, (expanded_right - expanded_left, expanded_bottom - expanded_top))\n",
    "                frame[expanded_top:expanded_bottom, expanded_left:expanded_right] = swap_image_resized\n",
    "            color = (0, 0, 255)  # Red\n",
    "        elif name == \"Candidate\":\n",
    "            color = (255, 0, 0)  # Blue\n",
    "        else:\n",
    "            color = (0, 255, 0)  # Green\n",
    "\n",
    "        if self.display_rectangle:\n",
    "            # Draw rectangle and annotations\n",
    "            cv2.rectangle(frame, (expanded_left, expanded_top), (expanded_right, expanded_bottom), color, 2)\n",
    "            cv2.putText(frame, name, (expanded_left, expanded_bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "            cv2.putText(frame, f\"Similarity: {similarity:.2f}\", (expanded_left, expanded_bottom + 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    def _time_str_to_seconds(self, time_str):\n",
    "        # Convert \"mm:ss\" format to total seconds\n",
    "        minutes, seconds = map(int, time_str.split(':'))\n",
    "        total_seconds = minutes * 60 + seconds\n",
    "        return total_seconds\n",
    "\n",
    "# Set the base image and target video\n",
    "base_image = \"../face_recognition/test_hanni2.jpg\"\n",
    "target_video = \"hanni.mp4\"\n",
    "\n",
    "# Define the segments to process (list of tuples with start time and duration in seconds)\n",
    "segments = [(\"00:00\", 10), (\"01:00\", 15)]  # Process from 0:00 for 10 seconds, and from 1:00 for 5 seconds\n",
    "\n",
    "# Create an instance of VideoFaceSwapper with the segments\n",
    "swapper = VideoFaceSwapper(base_image, target_video, output_video=\"output.webm\", display_video=True, display_rectangle=True, segments=segments)\n",
    "# If output_video ends with '.webm', it will be saved in WebM format.\n",
    "\n",
    "# Create an instance of FaceSwapper\n",
    "face_swapper = FaceSwapper(det_size=(320, 320))\n",
    "\n",
    "# Set the source face\n",
    "source_image = \"../face_recognition/faces/kimhs.jpg\"\n",
    "success = face_swapper.set_source_face(source_image)\n",
    "if not success:\n",
    "    print(\"Failed to set source face.\")\n",
    "    exit()\n",
    "\n",
    "@swapper.video_swap\n",
    "def swap_other_face(face_region):\n",
    "    swap_image = face_swapper.swap_faces_in_image(face_region)\n",
    "    return swap_image if swap_image is not None else face_region\n",
    "\n",
    "# Start the face swap process\n",
    "swap_other_face()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
