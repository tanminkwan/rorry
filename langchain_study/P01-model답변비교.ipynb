{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key를 환경변수로 관리하기 위한 설정 파일\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API Key 정보로드\n",
    "# - OPENAI_API_KEY = \"\"\n",
    "# - LANGCHAIN_TRACING_V2 = \"true\"\n",
    "# - LANGCHAIN_ENDPOINT = \"https://api.smith.langchain.com\"\n",
    "# - LANGCHAIN_API_KEY = \"\"\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain_study\" # Langsmith project 명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교대상 모델ID 상수 정의\n",
    "IS_HOME = False\n",
    "\n",
    "if IS_HOME:\n",
    "    AYA = \"aya:8b-23-q8_0\"\n",
    "    LLAMA3 = \"llama3:8b-instruct-q8_0\"\n",
    "    GEMMA = \"gemma:7b-instruct-q8_0\"\n",
    "    PHI3 = \"phi3:instruct\"\n",
    "else:\n",
    "    AYA = \"aya\"\n",
    "    LLAMA3 = \"llama3:instruct\"\n",
    "    GEMMA = \"gemma\"\n",
    "    PHI3 = \"phi3:instruct\"\n",
    "\n",
    "GPT = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm_aya = Ollama(model=AYA, temperature=0)\n",
    "llm_llama3 = Ollama(model=LLAMA3, temperature=0)\n",
    "llm_gemma = Ollama(model=GEMMA, temperature=0)\n",
    "llm_phi3 = Ollama(model=PHI3, temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_gpt = ChatOpenAI(model=GPT, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    System : 너는 다음 Instruction을 잘 수행하는 assistant 이다.\n",
    "    Instruction : {instruction}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_aya = prompt | llm_aya | output_parser\n",
    "chain_llama3 = prompt | llm_llama3 | output_parser\n",
    "chain_gemma = prompt | llm_gemma | output_parser\n",
    "chain_phi3 = prompt | llm_phi3 | output_parser\n",
    "chain_gpt = prompt | llm_gpt | output_parser\n",
    "\n",
    "param = {\n",
    "    AYA : chain_aya ,\n",
    "#    LLAMA3 : chain_llama3 ,\n",
    "#    GEMMA : chain_gemma ,\n",
    "#    PHI3 : chain_phi3 ,\n",
    "    GPT : chain_gpt ,\n",
    "}\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain_llms = RunnableParallel(**param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (TEST) 평가대상 LLM 문제 출제 및 답안 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aya': '식혜는 한국의 전통 음료로, 쌀로 만든 시럽이나 꿀로 단맛을 내고 때때로 마늘이나 생강과 같은 향신료를 넣어 만듭니다. '\n",
      "        '그것은 종종 차가운 상태로 제공되며, 달콤하고 약간 매콤한 맛이 납니다. 식혜는 한국 문화에서 인기 있는 음료이며, 특히 '\n",
      "        '여름에 인기가 많습니다. 그것은 종종 건강 증진과 소화 개선에 도움이 되는 것으로 여겨집니다.',\n",
      " 'gemma': '식혜는 한국에서 유명한 음료수입니다. 주로 과일, 특히 사과를 주원으로 하고, 설탕과 향이 들어간 음료입니다. 식혜는 '\n",
      "          '건강에 도움이 되고, 특히 여름에 마셔면 신선감을 느끼고 휴식을 취할 수 있습니다.',\n",
      " 'gpt-4o': '식혜는 한국의 전통 음료수로, 주로 쌀과 엿기름을 사용하여 만듭니다. 식혜는 달콤하고 시원한 맛이 특징이며, 특히 '\n",
      "           '명절이나 특별한 행사에서 자주 즐겨 마십니다. 다음은 식혜에 대한 주요 정보입니다:\\n'\n",
      "           '\\n'\n",
      "           '1. **재료**:\\n'\n",
      "           '   - **쌀**: 주로 찹쌀을 사용하지만, 멥쌀을 사용하기도 합니다.\\n'\n",
      "           '   - **엿기름**: 엿기름 가루를 물에 우려내어 사용합니다.\\n'\n",
      "           '   - **설탕**: 단맛을 더하기 위해 사용합니다.\\n'\n",
      "           '   - **물**: 기본적인 재료로 사용됩니다.\\n'\n",
      "           '\\n'\n",
      "           '2. **만드는 방법**:\\n'\n",
      "           '   1. 엿기름 가루를 물에 넣고 잘 섞은 후, 체에 걸러 엿기름 물을 만듭니다.\\n'\n",
      "           '   2. 찹쌀을 씻어 물에 불린 후, 밥을 짓습니다.\\n'\n",
      "           '   3. 지은 밥을 엿기름 물에 넣고 일정 시간 동안 발효시킵니다. 이 과정에서 밥알이 떠오르면 발효가 잘 된 '\n",
      "           '것입니다.\\n'\n",
      "           '   4. 발효가 끝나면 밥알을 건져내고, 설탕을 넣어 단맛을 조절합니다.\\n'\n",
      "           '   5. 식혜를 냉장고에 넣어 시원하게 한 후, 떠오른 밥알과 함께 서빙합니다.\\n'\n",
      "           '\\n'\n",
      "           '3. **효능**:\\n'\n",
      "           '   - 소화를 돕는 효능이 있어 식후 음료로 좋습니다.\\n'\n",
      "           '   - 엿기름에 포함된 효소가 소화 작용을 도와줍니다.\\n'\n",
      "           '   - 갈증 해소에 효과적입니다.\\n'\n",
      "           '\\n'\n",
      "           '4. **문화적 의미**:\\n'\n",
      "           '   - 식혜는 한국의 전통 명절인 설날과 추석에 자주 마시는 음료입니다.\\n'\n",
      "           '   - 전통적으로 가정에서 직접 만들어 마시기도 하지만, 현대에는 상점에서 쉽게 구입할 수 있는 제품도 많이 나와 '\n",
      "           '있습니다.\\n'\n",
      "           '\\n'\n",
      "           '식혜는 그 독특한 맛과 향으로 많은 사람들에게 사랑받는 음료입니다. 시원하게 마시면 더욱 맛있으며, 특히 더운 여름철에 '\n",
      "           '인기가 많습니다.',\n",
      " 'llama3:instruct': '😊\\n'\n",
      "                    '\\n'\n",
      "                    'So, you want to know about Sikhye (식혜), a traditional '\n",
      "                    'Korean drink. Sikhye is a sweet, fermented rice drink '\n",
      "                    \"that has been enjoyed for centuries in Korea. Here's what \"\n",
      "                    'I can tell you about it:\\n'\n",
      "                    '\\n'\n",
      "                    '**What is Sikhye?**\\n'\n",
      "                    'Sikhye is a type of Korean beverage made from fermented '\n",
      "                    'glutinous rice and various flavorings such as sugar, '\n",
      "                    'honey, or fruit extracts. The fermentation process gives '\n",
      "                    'the drink its unique sweet and slightly sour taste.\\n'\n",
      "                    '\\n'\n",
      "                    '**History**\\n'\n",
      "                    'Sikhye has been a part of Korean culture for over 1,000 '\n",
      "                    'years. It was originally consumed by royalty and nobles, '\n",
      "                    'but later became popular among common people as well. '\n",
      "                    'Sikhye was often served at special occasions like '\n",
      "                    'weddings and holidays.\\n'\n",
      "                    '\\n'\n",
      "                    '**Taste and Texture**\\n'\n",
      "                    'The taste of Sikhye is sweet and slightly sour, with a '\n",
      "                    'hint of fermentation flavor. The texture is thick and '\n",
      "                    'syrupy, similar to honey or molasses.\\n'\n",
      "                    '\\n'\n",
      "                    '**Variations**\\n'\n",
      "                    'There are many variations of Sikhye, depending on the '\n",
      "                    'type of rice used, the level of fermentation, and the '\n",
      "                    'added flavorings. Some common flavors include:\\n'\n",
      "                    '\\n'\n",
      "                    '* Original: Made with glutinous rice and sugar\\n'\n",
      "                    '* Fruit-flavored: Infused with fruit extracts like '\n",
      "                    'strawberry, orange, or grapefruit\\n'\n",
      "                    '* Honey-flavored: Made with honey instead of sugar\\n'\n",
      "                    '* Spiced: Adds spices like cinnamon, ginger, or nutmeg '\n",
      "                    'for extra flavor\\n'\n",
      "                    '\\n'\n",
      "                    '**Health Benefits**\\n'\n",
      "                    'Sikhye is believed to have several health benefits, '\n",
      "                    'including:\\n'\n",
      "                    '\\n'\n",
      "                    '* Aiding digestion and relieving stomach discomfort\\n'\n",
      "                    '* Providing antioxidants and vitamins\\n'\n",
      "                    '* Helping to regulate blood sugar levels\\n'\n",
      "                    '\\n'\n",
      "                    '**How to Enjoy Sikhye**\\n'\n",
      "                    'Sikhye can be enjoyed on its own as a refreshing drink or '\n",
      "                    \"used as an ingredient in cooking. It's often served warm \"\n",
      "                    'during the winter months, but it can also be chilled for '\n",
      "                    'a summer treat.\\n'\n",
      "                    '\\n'\n",
      "                    'I hope this helps you understand more about Sikhye! Do '\n",
      "                    'you have any specific questions or would you like to know '\n",
      "                    'more about Korean culture and traditions? 😊',\n",
      " 'phi3:instruct': ' 식혜라는 음료수은 한국의 고유입니다. 이 음료수는 청산물과 같은 맨바리를 기르기 위해 사연화 소화물로, '\n",
      "                  '따라서 청산물과 같은 영향을 주는 합성물입니다. 식혜라의 기타 정보는 다음으로 설명될 수 있습니다:\\n'\n",
      "                  '\\n'\n",
      "                  '- 화학 이름: 소화물 근비\\n'\n",
      "                  '- 가스 유클리드 수치: 1.02\\n'\n",
      "                  '- 시도 유탱자: 청산물\\n'\n",
      "                  '- 기타 정보: 식혜라는 한국의 고유입니다. 이 음료수는 청산물과 같은 영향을 주는 합성물입니다.\\n'\n",
      "                  '\\n'\n",
      "                  '식혜라는 한국의 고유입니다, 청산물과 같은 영향을 주는 합성물이며, 소화물 근비를 가진 화학 이름인 소화물 '\n",
      "                  '근비입니다. 식혜라의 유클리드 수치는 1.02입니다.\\n'\n",
      "                  '\\n'\n",
      "                  '또한, 식혜라는 음료수가 주로 한국에서 사용되어 있는 음료수로, 일부 음식에서 사용되는 합성물이며, '\n",
      "                  '청산물과 같은 영향을 주는 합성물입니다.\\n'\n",
      "                  ' Written in'}\n"
     ]
    }
   ],
   "source": [
    "# chain_llms 실행 Test\n",
    "responses = chain_llms.invoke({\"instruction\":\"식혜라는 음료수에 대해 알려주세요.\"})\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instruction = \"1990년대 한국 대중가요에 대해 알려주세요.\"\n",
    "instruction = \"필리핀의 대표적인 대중교통 알려줘\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CASE # 1. StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    System : \n",
    "    너는 llm model들의 답변을 비교하고 평가하는 AI 이다.\n",
    "    Instruction과 Responses 안의 각각의 llm별 응답을 \n",
    "    정확성(Accuracy), 관련성(Relevance), 유창성(Fluency), 완전성(Completeness) 측면에서    \n",
    "    분석하고 최고 점수 5점으로 0점 ~ 5점 사이 점수를 부여하라.\n",
    "\n",
    "    한국어로 답변해줘.\n",
    "\n",
    "    Instruction : {instruction}\n",
    "    Resonses : {responses}\n",
    "    \"\"\",\n",
    "    input_variables=[\"instruction\", \"responses\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain_combinded = (\n",
    "    {\"responses\" : chain_llms, \"instruction\" : RunnablePassthrough()}\n",
    "    | eval_prompt\n",
    "    | llm_gpt\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실행(invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 평가 기준\n",
      "1. **정확성(Accuracy)**: 제공된 정보가 사실에 부합하는지 여부.\n",
      "2. **관련성(Relevance)**: 질문에 대한 답변이 얼마나 관련성이 있는지.\n",
      "3. **유창성(Fluency)**: 문장이 얼마나 자연스럽고 읽기 쉬운지.\n",
      "4. **완전성(Completeness)**: 답변이 질문에 대해 얼마나 완전하게 설명하는지.\n",
      "\n",
      "### 평가 결과\n",
      "\n",
      "#### aya\n",
      "- **정확성**: 5점\n",
      "  - 쿠바 미사일 위기의 주요 사건과 결과를 정확하게 설명함.\n",
      "- **관련성**: 5점\n",
      "  - 질문에 대한 답변이 매우 관련성이 높음.\n",
      "- **유창성**: 5점\n",
      "  - 문장이 자연스럽고 읽기 쉬움.\n",
      "- **완전성**: 5점\n",
      "  - 사건의 배경, 전개, 결과를 모두 포함하여 완전하게 설명함.\n",
      "\n",
      "#### llama3:instruct\n",
      "- **정확성**: 5점\n",
      "  - 쿠바 미사일 위기의 주요 사건과 결과를 정확하게 설명함.\n",
      "- **관련성**: 5점\n",
      "  - 질문에 대한 답변이 매우 관련성이 높음.\n",
      "- **유창성**: 5점\n",
      "  - 문장이 자연스럽고 읽기 쉬움.\n",
      "- **완전성**: 5점\n",
      "  - 사건의 배경, 전개, 결과를 모두 포함하여 완전하게 설명함.\n",
      "\n",
      "#### gemma\n",
      "- **정확성**: 2점\n",
      "  - 쿠바 미사일 위기와 관련된 정보가 부정확하고, 쿠바의 외교 정책에 대한 설명이 주를 이룸.\n",
      "- **관련성**: 2점\n",
      "  - 질문에 대한 답변이 다소 관련성이 떨어짐.\n",
      "- **유창성**: 4점\n",
      "  - 문장은 자연스럽고 읽기 쉬움.\n",
      "- **완전성**: 2점\n",
      "  - 쿠바 미사일 위기의 주요 사건과 결과에 대한 설명이 부족함.\n",
      "\n",
      "#### phi3:instruct\n",
      "- **정확성**: 0점\n",
      "  - 제공된 정보가 쿠바 미사일 위기와 전혀 관련이 없음.\n",
      "- **관련성**: 0점\n",
      "  - 질문에 대한 답변이 전혀 관련성이 없음.\n",
      "- **유창성**: 1점\n",
      "  - 문장이 반복적이고 읽기 어려움.\n",
      "- **완전성**: 0점\n",
      "  - 쿠바 미사일 위기에 대한 설명이 전혀 없음.\n",
      "\n",
      "#### gpt-4o\n",
      "- **정확성**: 5점\n",
      "  - 쿠바 미사일 위기의 주요 사건과 결과를 정확하게 설명함.\n",
      "- **관련성**: 5점\n",
      "  - 질문에 대한 답변이 매우 관련성이 높음.\n",
      "- **유창성**: 5점\n",
      "  - 문장이 자연스럽고 읽기 쉬움.\n",
      "- **완전성**: 5점\n",
      "  - 사건의 배경, 전개, 결과를 모두 포함하여 완전하게 설명함.\n",
      "\n",
      "### 종합 평가\n",
      "- **aya**: 5점\n",
      "- **llama3:instruct**: 5점\n",
      "- **gemma**: 2.5점\n",
      "- **phi3:instruct**: 0.25점\n",
      "- **gpt-4o**: 5점\n",
      "\n",
      "**최종 평가**: aya, llama3:instruct, gpt-4o가 가장 높은 점수를 받았으며, 쿠바 미사일 위기에 대해 정확하고 완전한 설명을 제공했습니다.\n"
     ]
    }
   ],
   "source": [
    "response = chain_combinded.invoke({\"instruction\":instruction})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CASE # 2. PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class EvaluationByModel(BaseModel):\n",
    "    model_id: str = Field(description=\"LLM 모델 이름 또는 LLM 모델 ID\")\n",
    "    accuracy_eval: str = Field(description=\"정확성(Accuracy) 평가\")\n",
    "    accuracy_score: int = Field(description=\"정확성(Accuracy) 평가 점수\")\n",
    "    relevance_eval: str = Field(description=\"관련성(Relevance) 평가\")\n",
    "    relevance_score: int = Field(description=\"관련성(Relevance) 평가 점수\")\n",
    "    fluency_eval: str = Field(description=\"유창성(Fluency) 평가\")\n",
    "    fluency_score: int = Field(description=\"유창성(Fluency) 평가 점수\")\n",
    "    completeness_eval: str = Field(description=\"완전성(Completeness) 평가\")\n",
    "    completeness_score: int = Field(description=\"관련성(Relevance) 평가 점수\")\n",
    "\n",
    "class EvaluationResponse(BaseModel):\n",
    "    instruction: str = Field(description=\"Instruction 내용 원문\")\n",
    "    accuracy: str = Field(description=\"정확성(Accuracy) 평가 기준\")\n",
    "    relevance: str = Field(description=\"관련성(Relevance) 평가 기준\")\n",
    "    fluency: str = Field(description=\"유창성(Fluency) 평가 기준\")\n",
    "    completeness: str = Field(description=\"완전성(Completeness) 평가 기준\")\n",
    "    evaluation_by_model: List[EvaluationByModel] = Field(description=\"LLM 모델별 세부 평가 내용\")\n",
    "    overall: str = Field(description=\"종합평가\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The function `convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.3.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'EvaluationResponse',\n",
       " 'description': '',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'instruction': {'description': 'Instruction 내용 원문',\n",
       "    'type': 'string'},\n",
       "   'accuracy': {'description': '정확성(Accuracy) 평가 기준', 'type': 'string'},\n",
       "   'relevance': {'description': '관련성(Relevance) 평가 기준', 'type': 'string'},\n",
       "   'fluency': {'description': '유창성(Fluency) 평가 기준', 'type': 'string'},\n",
       "   'completeness': {'description': '완전성(Completeness) 평가 기준',\n",
       "    'type': 'string'},\n",
       "   'evaluation_by_model': {'description': 'LLM 모델별 세부 평가 내용',\n",
       "    'type': 'array',\n",
       "    'items': {'type': 'object',\n",
       "     'properties': {'model_id': {'description': 'LLM 모델 이름 또는 LLM 모델 ID',\n",
       "       'type': 'string'},\n",
       "      'accuracy_eval': {'description': '정확성(Accuracy) 평가', 'type': 'string'},\n",
       "      'accuracy_score': {'description': '정확성(Accuracy) 평가 점수',\n",
       "       'type': 'integer'},\n",
       "      'relevance_eval': {'description': '관련성(Relevance) 평가', 'type': 'string'},\n",
       "      'relevance_score': {'description': '관련성(Relevance) 평가 점수',\n",
       "       'type': 'integer'},\n",
       "      'fluency_eval': {'description': '유창성(Fluency) 평가', 'type': 'string'},\n",
       "      'fluency_score': {'description': '유창성(Fluency) 평가 점수',\n",
       "       'type': 'integer'},\n",
       "      'completeness_eval': {'description': '완전성(Completeness) 평가',\n",
       "       'type': 'string'},\n",
       "      'completeness_score': {'description': '관련성(Relevance) 평가 점수',\n",
       "       'type': 'integer'}},\n",
       "     'required': ['model_id',\n",
       "      'accuracy_eval',\n",
       "      'accuracy_score',\n",
       "      'relevance_eval',\n",
       "      'relevance_score',\n",
       "      'fluency_eval',\n",
       "      'fluency_score',\n",
       "      'completeness_eval',\n",
       "      'completeness_score']}},\n",
       "   'overall': {'description': '종합평가', 'type': 'string'}},\n",
       "  'required': ['instruction',\n",
       "   'accuracy',\n",
       "   'relevance',\n",
       "   'fluency',\n",
       "   'completeness',\n",
       "   'evaluation_by_model',\n",
       "   'overall']}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schema 확인\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "f = convert_pydantic_to_openai_function(EvaluationResponse)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "p_parser = PydanticOutputParser(pydantic_object=EvaluationResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    System : \n",
    "    너는 llm model들의 답변을 비교하고 평가하는 AI 이다.\n",
    "    Instruction과 Responses 안의 각각의 llm별 응답을\n",
    "    정확성(Accuracy), 관련성(Relevance), 유창성(Fluency), 완전성(Completeness) 측면에서    \n",
    "    분석하고 최고 점수 5점으로 0점 ~ 5점 사이 점수를 부여하라.\n",
    "\n",
    "    한국어로 답변해줘.\n",
    "    Format 에 맞춰서 답변해줘\n",
    "\n",
    "    Instruction : {instruction}\n",
    "    Responses : {responses}\n",
    "\n",
    "    Format : \n",
    "    {format}\n",
    "    \"\"\",\n",
    "    input_variables=[\"instruction\", \"responses\"],\n",
    "    partial_variables={\"format\" : p_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain_combinded = (\n",
    "    {\"responses\" : chain_llms, \"instruction\" : RunnablePassthrough()}\n",
    "    | eval_prompt \n",
    "    | llm_gpt\n",
    "    | p_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실행(invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host '70.10.15.10'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host '70.10.15.10'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host '70.10.15.10'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host '70.10.15.10'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction='필리핀의 대표적인 대중교통 알려줘' accuracy='각 모델의 응답이 필리핀의 대표적인 대중교통 수단을 정확하게 설명하는지 평가합니다.' relevance='응답이 주어진 질문과 얼마나 관련성이 있는지 평가합니다.' fluency='응답이 문법적으로 얼마나 유창하게 작성되었는지 평가합니다.' completeness='응답이 질문에 대해 얼마나 완전하게 답변했는지 평가합니다.' evaluation_by_model=[EvaluationByModel(model_id='aya', accuracy_eval='지프니, 버스, 트라이시클, 보트, 기차 등 필리핀의 주요 대중교통 수단을 정확하게 설명했습니다.', accuracy_score=5, relevance_eval='응답이 질문과 매우 관련성이 높습니다.', relevance_score=5, fluency_eval='문법적으로 유창하게 작성되었습니다.', fluency_score=5, completeness_eval='필리핀의 다양한 대중교통 수단을 포괄적으로 설명했습니다.', completeness_score=5), EvaluationByModel(model_id='gpt-4o', accuracy_eval='지프니, 트라이시클, 페디캡, 버스, 택시, UV 익스프레스, LRT/MRT 등 필리핀의 주요 대중교통 수단을 정확하게 설명했습니다.', accuracy_score=5, relevance_eval='응답이 질문과 매우 관련성이 높습니다.', relevance_score=5, fluency_eval='문법적으로 유창하게 작성되었습니다.', fluency_score=5, completeness_eval='필리핀의 다양한 대중교통 수단을 포괄적으로 설명했습니다.', completeness_score=5)] overall='두 모델 모두 정확성, 관련성, 유창성, 완전성 측면에서 매우 우수한 응답을 제공했습니다.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host '70.10.15.10'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "response = chain_combinded.invoke({\"instruction\":instruction})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 질문과 평가결과를 DB에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiffanie.kim\\AppData\\Local\\Temp\\ipykernel_10440\\2944376351.py:5: MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import Session, relationship, sessionmaker\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "engine = create_engine(\"sqlite://\", echo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CASE #1. 명시적으로 Table 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String, Text, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship, sessionmaker\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class TEvaluationResponse(Base):\n",
    "    __tablename__ = 'evaluation_responses'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    instruction = Column(Text, nullable=False)\n",
    "    accuracy = Column(Text, nullable=False)\n",
    "    relevance = Column(Text, nullable=False)\n",
    "    fluency = Column(Text, nullable=False)\n",
    "    completeness = Column(Text, nullable=False)\n",
    "    overall = Column(Text, nullable=False)\n",
    "    \n",
    "    evaluations = relationship(\"TEvaluationByModel\", back_populates=\"response\")\n",
    "\n",
    "class TEvaluationByModel(Base):\n",
    "    __tablename__ = 'evaluation_by_model'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    model_id = Column(String, nullable=False)\n",
    "    accuracy_eval = Column(Text, nullable=False)\n",
    "    accuracy_score = Column(Integer, nullable=False)\n",
    "    relevance_eval = Column(Text, nullable=False)\n",
    "    relevance_score = Column(Integer, nullable=False)\n",
    "    fluency_eval = Column(Text, nullable=False)\n",
    "    fluency_score = Column(Integer, nullable=False)\n",
    "    completeness_eval = Column(Text, nullable=False)\n",
    "    completeness_score = Column(Integer, nullable=False)\n",
    "    \n",
    "    response_id = Column(Integer, ForeignKey('evaluation_responses.id'))\n",
    "    response = relationship(\"TEvaluationResponse\", back_populates=\"evaluations\")\n",
    "\n",
    "# Create an engine and a session\n",
    "engine = create_engine('sqlite:///evaluations.db')\n",
    "Session = sessionmaker(bind=engine)\n",
    "Base.metadata.create_all(engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (Error 남)CASE #2. Pydantic Schema를 이용하여 동적으로 Table 만들기 (ChatGPT가 만들어 줌)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String, Text, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship, sessionmaker\n",
    "from typing import Dict, Type\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "# Mapping Pydantic types to SQLAlchemy types\n",
    "type_mapping = {\n",
    "    str: String,\n",
    "    int: Integer,\n",
    "    float: Text,\n",
    "}\n",
    "\n",
    "def create_sqlalchemy_model(pydantic_model: Type[BaseModel], base: Type[Base], table_name: str, exclude_fields: List[str] = None, additional_columns: Dict[str, Type[Column]] = None):\n",
    "    if exclude_fields is None:\n",
    "        exclude_fields = []\n",
    "    if additional_columns is None:\n",
    "        additional_columns = {}\n",
    "\n",
    "    fields = pydantic_model.__fields__\n",
    "    columns = {\n",
    "        '__tablename__': table_name,\n",
    "        'id': Column(Integer, primary_key=True, autoincrement=True)\n",
    "    }\n",
    "    \n",
    "    for field_name, field in fields.items():\n",
    "        if field_name in exclude_fields:\n",
    "            continue\n",
    "        field_type = field.outer_type_\n",
    "        columns[field_name] = Column(type_mapping[field_type], nullable=False)\n",
    "    \n",
    "    # Add additional columns for relationships or other special cases\n",
    "    columns.update(additional_columns)\n",
    "    \n",
    "    # Create the new model class\n",
    "    model = type(table_name, (base,), columns)\n",
    "    return model\n",
    "\n",
    "# Dynamically create SQLAlchemy models without relationships\n",
    "TEvaluationResponse = create_sqlalchemy_model(\n",
    "    EvaluationResponse, \n",
    "    Base, \n",
    "    'evaluation_responses', \n",
    "    exclude_fields=['evaluation_by_model']\n",
    ")\n",
    "TEvaluationByModel = create_sqlalchemy_model(\n",
    "    EvaluationByModel, \n",
    "    Base, \n",
    "    'evaluation_by_model'\n",
    ")\n",
    "\n",
    "# Define relationships after both classes have been created\n",
    "TEvaluationResponse.evaluations = relationship(\"TEvaluationByModel\", back_populates=\"response\")\n",
    "TEvaluationByModel.response_id = Column(Integer, ForeignKey('evaluation_responses.id'))\n",
    "TEvaluationByModel.response = relationship(\"TEvaluationResponse\", back_populates=\"evaluations\")\n",
    "\n",
    "# Create an engine and a session\n",
    "engine = create_engine('sqlite:///evaluations.db')\n",
    "Session = sessionmaker(bind=engine)\n",
    "Base.metadata.create_all(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation_response(session, evaluation_response):\n",
    "\n",
    "    response_dict = evaluation_response.dict(exclude={\"evaluation_by_model\"})\n",
    "    print(response_dict)\n",
    "    response = TEvaluationResponse(\n",
    "        **response_dict  # Unpack the dictionary into keyword arguments\n",
    "    )\n",
    "    session.add(response)\n",
    "    session.flush()\n",
    "\n",
    "    for eval_model in evaluation_response.evaluation_by_model:\n",
    "        model_evaluation = TEvaluationByModel(\n",
    "            response_id=response.id,  # Use the generated ID\n",
    "            **eval_model.dict()  # Unpack the dictionary into keyword arguments\n",
    "        )\n",
    "        session.add(model_evaluation)\n",
    "    \n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': '필리핀의 대표적인 대중교통 알려줘', 'accuracy': '각 모델의 응답이 필리핀의 대표적인 대중교통 수단을 정확하게 설명하는지 평가합니다.', 'relevance': '응답이 주어진 질문과 얼마나 관련성이 있는지 평가합니다.', 'fluency': '응답이 문법적으로 얼마나 유창하게 작성되었는지 평가합니다.', 'completeness': '응답이 질문에 대해 얼마나 완전하게 답변했는지 평가합니다.', 'overall': '두 모델 모두 정확성, 관련성, 유창성, 완전성 측면에서 매우 우수한 응답을 제공했습니다.'}\n"
     ]
    }
   ],
   "source": [
    "session = Session()\n",
    "save_evaluation_response(session, response)\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
